{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NEMWEB Custom Data Source Demo\n",
    "\n",
    "This notebook demonstrates how to use the NEMWEB custom PySpark data source from the `src` folder.\n",
    "\n",
    "**Requirements:**\n",
    "- Databricks Runtime 15.4+ or Serverless (Environment Version 4)\n",
    "- Python Data Source API (GA in Spark 4.0)\n",
    "\n",
    "**What this notebook covers:**\n",
    "1. Installing the package from the src folder\n",
    "2. Registering the custom data source\n",
    "3. Batch reading from NEMWEB API\n",
    "4. Exploring the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup - Install Package from src Folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install the nemweb package from the src folder\n",
    "# This makes the nemweb_datasource module available for import\n",
    "%pip install -e /Workspace/Repos/{your-username}/databricks-nemweb-lab/databricks-nemweb-lab/src --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Restart Python to pick up the installed package\n",
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Register the Custom Data Source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from databricks.sdk.runtime import spark, dbutils\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Import the custom data source\n",
    "from nemweb_datasource import NemwebDataSource\n",
    "\n",
    "# Register with Spark - this enables spark.read.format(\"nemweb\")\n",
    "spark.dataSource.register(NemwebDataSource)\n",
    "\n",
    "print(\"NEMWEB data source registered successfully!\")\n",
    "print(\"You can now use: spark.read.format('nemweb')...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Batch Read - Fetch Data from NEMWEB API\n",
    "\n",
    "The NEMWEB data source fetches real electricity market data from AEMO's public API.\n",
    "\n",
    "**Available options:**\n",
    "- `table`: MMS table name (default: `DISPATCHREGIONSUM`)\n",
    "- `regions`: Comma-separated NEM regions (default: `NSW1,QLD1,SA1,VIC1,TAS1`)\n",
    "- `start_date`: Start date in `YYYY-MM-DD` format\n",
    "- `end_date`: End date in `YYYY-MM-DD` format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use yesterday's date (guaranteed to exist in NEMWEB CURRENT folder)\n",
    "yesterday = (datetime.now() - timedelta(days=1)).strftime(\"%Y-%m-%d\")\n",
    "\n",
    "# Read a single region for a single day (fast demo)\n",
    "df = (spark.read\n",
    "      .format(\"nemweb\")\n",
    "      .option(\"table\", \"DISPATCHREGIONSUM\")\n",
    "      .option(\"regions\", \"NSW1\")\n",
    "      .option(\"start_date\", yesterday)\n",
    "      .option(\"end_date\", yesterday)\n",
    "      .load())\n",
    "\n",
    "print(f\"Fetched {df.count()} rows for NSW1 on {yesterday}\")\n",
    "print(f\"Expected: ~288 rows (24 hours Ã— 12 intervals/hour at 5-min granularity)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the schema\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display sample data\n",
    "display(df.orderBy(\"SETTLEMENTDATE\").limit(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Read Multiple Regions\n",
    "\n",
    "The data source creates one partition per region, enabling parallel reads."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read all 5 NEM regions\n",
    "df_all_regions = (spark.read\n",
    "                  .format(\"nemweb\")\n",
    "                  .option(\"regions\", \"NSW1,QLD1,SA1,VIC1,TAS1\")\n",
    "                  .option(\"start_date\", yesterday)\n",
    "                  .option(\"end_date\", yesterday)\n",
    "                  .load())\n",
    "\n",
    "print(f\"Total rows: {df_all_regions.count()}\")\n",
    "print(f\"Partitions: {df_all_regions.rdd.getNumPartitions()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check row counts by region\n",
    "display(\n",
    "    df_all_regions\n",
    "    .groupBy(\"REGIONID\")\n",
    "    .count()\n",
    "    .orderBy(\"REGIONID\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Analyze the Data\n",
    "\n",
    "Run some basic analytics on the electricity market data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, avg, max, min, sum\n",
    "\n",
    "# Regional demand summary\n",
    "summary = (df_all_regions\n",
    "           .groupBy(\"REGIONID\")\n",
    "           .agg(\n",
    "               avg(\"TOTALDEMAND\").alias(\"avg_demand_mw\"),\n",
    "               max(\"TOTALDEMAND\").alias(\"peak_demand_mw\"),\n",
    "               min(\"TOTALDEMAND\").alias(\"min_demand_mw\"),\n",
    "               avg(\"AVAILABLEGENERATION\").alias(\"avg_generation_mw\")\n",
    "           )\n",
    "           .orderBy(col(\"avg_demand_mw\").desc()))\n",
    "\n",
    "display(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import hour\n",
    "\n",
    "# Hourly demand pattern for NSW\n",
    "hourly_demand = (df_all_regions\n",
    "                 .filter(col(\"REGIONID\") == \"NSW1\")\n",
    "                 .withColumn(\"hour\", hour(\"SETTLEMENTDATE\"))\n",
    "                 .groupBy(\"hour\")\n",
    "                 .agg(avg(\"TOTALDEMAND\").alias(\"avg_demand_mw\"))\n",
    "                 .orderBy(\"hour\"))\n",
    "\n",
    "display(hourly_demand)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Save to Delta Table (Optional)\n",
    "\n",
    "Persist the data to a Delta table for downstream analytics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to save to a Delta table\n",
    "# catalog = \"main\"\n",
    "# schema = \"nemweb_lab\"\n",
    "# table = \"dispatch_region_sum\"\n",
    "\n",
    "# # Create schema if not exists\n",
    "# spark.sql(f\"CREATE SCHEMA IF NOT EXISTS {catalog}.{schema}\")\n",
    "\n",
    "# # Write to Delta table\n",
    "# (df_all_regions\n",
    "#  .write\n",
    "#  .mode(\"append\")\n",
    "#  .saveAsTable(f\"{catalog}.{schema}.{table}\"))\n",
    "\n",
    "# print(f\"Data saved to {catalog}.{schema}.{table}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Multi-Day Read with Checkpointing (Advanced)\n",
    "\n",
    "For production workloads, use checkpointing to track progress and enable resumability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read a week of data with checkpoint tracking\n",
    "# This creates partitions for each (region, date) combination\n",
    "\n",
    "# week_ago = (datetime.now() - timedelta(days=7)).strftime(\"%Y-%m-%d\")\n",
    "\n",
    "# df_week = (spark.read\n",
    "#            .format(\"nemweb\")\n",
    "#            .option(\"regions\", \"NSW1,VIC1\")\n",
    "#            .option(\"start_date\", week_ago)\n",
    "#            .option(\"end_date\", yesterday)\n",
    "#            .option(\"checkpoint_table\", \"main.nemweb_lab.checkpoints\")  # Enable checkpointing\n",
    "#            .load())\n",
    "\n",
    "# print(f\"Partitions: {df_week.rdd.getNumPartitions()}\")\n",
    "# print(f\"Total rows: {df_week.count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "You've successfully:\n",
    "1. Installed the NEMWEB package from the src folder\n",
    "2. Registered the custom data source with Spark\n",
    "3. Fetched real electricity market data from the NEMWEB API\n",
    "4. Analyzed demand patterns across NEM regions\n",
    "\n",
    "**Next Steps:**\n",
    "- Explore the Lakeflow Pipeline notebook to build a bronze/silver/gold medallion architecture\n",
    "- Check out the streaming capabilities for real-time ingestion\n",
    "- Review the `nemweb_utils.py` for retry logic and error handling patterns"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

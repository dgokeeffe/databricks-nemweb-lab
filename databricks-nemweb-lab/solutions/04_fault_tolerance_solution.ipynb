{
 "cells": [
  {
   "cell_type": "markdown",
   "source": "# Solution: Exercise 4 - Fault Tolerance & Recovery\nComplete solutions for the fault tolerance exercise.\n",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "\nfrom pyspark.sql.functions import col, current_timestamp, lit, struct, to_json\nfrom pyspark.sql.types import StructType, StructField, StringType, TimestampType\nfrom datetime import datetime\nimport time\nimport hashlib\n\n# Import spark from Databricks SDK for IDE support and local development\nfrom databricks.sdk.runtime import spark\n",
   "metadata": {},
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "\n# Configuration widgets\ndbutils.widgets.text(\"catalog\", \"workspace\", \"Catalog Name\")\n",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "\ndbutils.widgets.text(\"schema\", \"nemweb_lab\", \"Schema Name\")\n",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "\nCATALOG = dbutils.widgets.get(\"catalog\")\nSCHEMA = dbutils.widgets.get(\"schema\")\n\nprint(f\"Using catalog: {CATALOG}, schema: {SCHEMA}\")\n",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "",
   "metadata": {},
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "## Solution 4.1: Retry with Exponential Backoff\n",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "\ndef fetch_with_retry(url: str, max_retries: int = 3, base_delay: float = 1.0) -> bytes:\n    \"\"\"\n    Fetch URL with exponential backoff retry.\n\n    SOLUTION 4.1: Complete retry implementation\n    \"\"\"\n    import urllib.request\n    from urllib.error import HTTPError, URLError\n\n    last_error = None\n\n    for attempt in range(max_retries):\n        try:\n            request = urllib.request.Request(\n                url,\n                headers={\"User-Agent\": \"NemwebLab/1.0\"}\n            )\n            with urllib.request.urlopen(request, timeout=30) as response:\n                return response.read()\n\n        except (HTTPError, URLError) as e:\n            last_error = e\n\n            # SOLUTION 4.1a: Don't retry on 404\n            if isinstance(e, HTTPError) and e.code == 404:\n                raise  # 404 means data doesn't exist, don't retry\n\n            if attempt < max_retries - 1:\n                # SOLUTION 4.1b: Calculate delay with exponential backoff\n                delay = base_delay * (2 ** attempt)\n\n                print(f\"Attempt {attempt + 1}/{max_retries} failed: {e}\")\n                print(f\"Retrying in {delay:.1f} seconds...\")\n\n                # SOLUTION 4.1c: Wait before retry\n                time.sleep(delay)\n\n    raise last_error\n\n\n# Test retry logic\nprint(\"Testing retry logic with 503 endpoint...\")\ntest_url = \"https://httpstat.us/503\"\ntry:\n    fetch_with_retry(test_url, max_retries=2, base_delay=0.5)\nexcept Exception as e:\n    print(f\"Expected failure after retries: {type(e).__name__}\")\n\nprint(\"\\nTesting 404 (should not retry)...\")\ntest_url_404 = \"https://httpstat.us/404\"\ntry:\n    fetch_with_retry(test_url_404, max_retries=3, base_delay=1.0)\nexcept Exception as e:\n    print(f\"Immediate failure (no retry): {type(e).__name__}\")\n",
   "metadata": {},
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "",
   "metadata": {},
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "## Solution 4.2: Checkpoint-Based Recovery\n",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "\n# Create checkpoint table\ncheckpoint_table = f\"{CATALOG}.{SCHEMA}.nemweb_checkpoints_solution\"\n\ncheckpoint_schema = StructType([\n    StructField(\"partition_id\", StringType(), False),\n    StructField(\"region\", StringType(), True),\n    StructField(\"start_date\", StringType(), True),\n    StructField(\"end_date\", StringType(), True),\n    StructField(\"completed_at\", TimestampType(), True),\n    StructField(\"row_count\", StringType(), True),\n])\n\nspark.createDataFrame([], checkpoint_schema).write.format(\"delta\").mode(\"ignore\").saveAsTable(checkpoint_table)\n",
   "metadata": {},
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "\ndef get_completed_partitions(checkpoint_table: str) -> set:\n    \"\"\"\n    Load completed partition IDs from checkpoint table.\n\n    SOLUTION 4.2a: Query checkpoint table\n    \"\"\"\n    try:\n        df = spark.read.table(checkpoint_table)\n        # SOLUTION: Extract partition_ids as a set\n        return set(\n            row.partition_id\n            for row in df.select(\"partition_id\").collect()\n        )\n    except Exception as e:\n        print(f\"Could not read checkpoints: {e}\")\n        return set()\n\n\ndef mark_partition_complete(\n    checkpoint_table: str,\n    partition_id: str,\n    region: str,\n    start_date: str,\n    end_date: str,\n    row_count: int\n):\n    \"\"\"\n    Mark a partition as completed in the checkpoint table.\n\n    SOLUTION 4.2b: Insert checkpoint record\n    \"\"\"\n    checkpoint_record = spark.createDataFrame([{\n        \"partition_id\": partition_id,\n        \"region\": region,\n        \"start_date\": start_date,\n        \"end_date\": end_date,\n        \"completed_at\": datetime.now(),\n        \"row_count\": str(row_count),\n    }])\n\n    # SOLUTION: Append to checkpoint table\n    checkpoint_record.write.format(\"delta\").mode(\"append\").saveAsTable(checkpoint_table)\n\n\ndef read_with_checkpoints(checkpoint_table: str, regions: list, start_date: str, end_date: str):\n    \"\"\"\n    Read NEMWEB data, skipping already-completed partitions.\n\n    SOLUTION 4.2c: Filter completed partitions\n    \"\"\"\n    completed = get_completed_partitions(checkpoint_table)\n    print(f\"Found {len(completed)} completed partitions\")\n\n    processed_count = 0\n\n    for region in regions:\n        # Generate partition ID (same logic as NemwebPartition)\n        id_string = f\"DISPATCHREGIONSUM:{region}:{start_date}:{end_date}\"\n        partition_id = hashlib.md5(id_string.encode()).hexdigest()[:12]\n\n        # SOLUTION 4.2c: Skip if already completed\n        if partition_id in completed:\n            print(f\"Skipping partition {partition_id} ({region}) - already completed\")\n            continue\n\n        print(f\"Processing partition {partition_id} ({region})\")\n\n        # Simulate processing\n        row_count = 100  # Would be actual row count\n\n        # Mark complete after successful processing\n        mark_partition_complete(\n            checkpoint_table, partition_id, region,\n            start_date, end_date, row_count\n        )\n        processed_count += 1\n\n    print(f\"Processed {processed_count} partitions\")\n    return processed_count\n\n\n# Test checkpoint logic\nprint(\"=== First run ===\")\ntest_regions = [\"NSW1\", \"VIC1\", \"QLD1\"]\ncount1 = read_with_checkpoints(checkpoint_table, test_regions, \"2024-01-01\", \"2024-01-07\")\n\nprint(\"\\n=== Second run (should skip all) ===\")\ncount2 = read_with_checkpoints(checkpoint_table, test_regions, \"2024-01-01\", \"2024-01-07\")\n\nprint(f\"\\nFirst run: {count1} partitions, Second run: {count2} partitions\")\n",
   "metadata": {},
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "",
   "metadata": {},
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "## Solution 4.3: Streaming Configuration\nThe streaming reader is implemented in `nemweb_datasource.py` as `NemwebStreamReader`.\n",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "\n# Complete streaming pipeline example\nstreaming_example = \"\"\"\n# Production streaming pipeline with automatic recovery\n\nfrom pyspark.sql.functions import col, current_timestamp\n\n# Register the data source\nspark.dataSource.register(NemwebDataSource)\n\n# Start streaming query\nquery = (spark.readStream\n    .format(\"nemweb\")\n    .option(\"table\", \"DISPATCHREGIONSUM\")\n    .option(\"regions\", \"NSW1,VIC1,QLD1,SA1,TAS1\")\n    .load()\n    # Add processing timestamp\n    .withColumn(\"_processed_at\", current_timestamp())\n    .writeStream\n    .format(\"delta\")\n    .outputMode(\"append\")\n    # Spark manages offsets in this location\n    .option(\"checkpointLocation\", \"/checkpoints/nemweb_production\")\n    # Process every 5 minutes\n    .trigger(processingTime=\"5 minutes\")\n    .toTable(\"nemweb_bronze\"))\n\n# Monitor the query\nprint(f\"Query ID: {query.id}\")\nprint(f\"Status: {query.status}\")\n\n# Recovery scenarios:\n# 1. Stop query: query.stop()\n#    - Commits current offset\n#    - Next start resumes from committed offset\n#\n# 2. Cluster crash:\n#    - Restart cluster, run same code\n#    - Spark reads last committed offset from checkpointLocation\n#    - Resumes processing from where it left off\n#\n# 3. Code change:\n#    - Usually safe to restart with same checkpointLocation\n#    - Schema changes may require new checkpoint\n\"\"\"\n\nprint(streaming_example)\n",
   "metadata": {},
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "",
   "metadata": {},
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "## Solution 4.4: Dead Letter Queue\n",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "\n# Create DLQ table\ndlq_table = f\"{CATALOG}.{SCHEMA}.nemweb_dlq_solution\"\n\ndlq_schema = StructType([\n    StructField(\"SETTLEMENTDATE\", StringType(), True),\n    StructField(\"REGIONID\", StringType(), True),\n    StructField(\"TOTALDEMAND\", StringType(), True),\n    StructField(\"_error_reason\", StringType(), True),\n    StructField(\"_error_timestamp\", TimestampType(), True),\n    StructField(\"_original_data\", StringType(), True),\n])\n\nspark.createDataFrame([], dlq_schema).write.format(\"delta\").mode(\"ignore\").saveAsTable(dlq_table)\n",
   "metadata": {},
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "\ndef process_with_dlq(df, dlq_table: str):\n    \"\"\"\n    Process DataFrame, routing bad records to DLQ.\n\n    SOLUTION 4.4: Complete DLQ implementation\n    \"\"\"\n    # Define what makes a record \"bad\"\n    is_valid = (\n        col(\"REGIONID\").isNotNull() &\n        col(\"REGIONID\").isin(\"NSW1\", \"VIC1\", \"QLD1\", \"SA1\", \"TAS1\") &\n        (col(\"TOTALDEMAND\").cast(\"double\") > 0)\n    )\n\n    # SOLUTION 4.4a: Split into good and bad records\n    good_records = df.filter(is_valid)\n    bad_records = df.filter(~is_valid)\n\n    bad_count = bad_records.count()\n\n    if bad_count > 0:\n        # SOLUTION 4.4b: Add error metadata to bad records\n        bad_with_metadata = (bad_records\n            .withColumn(\"_error_reason\",\n                when(col(\"REGIONID\").isNull(), \"NULL_REGION\")\n                .when(~col(\"REGIONID\").isin(\"NSW1\", \"VIC1\", \"QLD1\", \"SA1\", \"TAS1\"), \"INVALID_REGION\")\n                .when(col(\"TOTALDEMAND\").cast(\"double\") <= 0, \"INVALID_DEMAND\")\n                .otherwise(\"UNKNOWN\"))\n            .withColumn(\"_error_timestamp\", current_timestamp())\n            .withColumn(\"_original_data\", to_json(struct(\n                col(\"SETTLEMENTDATE\"),\n                col(\"REGIONID\"),\n                col(\"TOTALDEMAND\")\n            )))\n        )\n\n        # SOLUTION 4.4c: Write bad records to DLQ table\n        (bad_with_metadata\n            .select(\"SETTLEMENTDATE\", \"REGIONID\", \"TOTALDEMAND\",\n                    \"_error_reason\", \"_error_timestamp\", \"_original_data\")\n            .write\n            .format(\"delta\")\n            .mode(\"append\")\n            .saveAsTable(dlq_table))\n\n        print(f\"Routed {bad_count} bad records to DLQ: {dlq_table}\")\n\n    good_count = good_records.count()\n    print(f\"Good records: {good_count}\")\n\n    return good_records\n\n\n# Need to import when for the solution\nfrom pyspark.sql.functions import when\n\n# Test with sample data including bad records\ntest_data = [\n    (\"2024-01-01 00:05:00\", \"NSW1\", \"7500.5\"),  # Good\n    (\"2024-01-01 00:05:00\", \"VIC1\", \"5200.3\"),  # Good\n    (\"2024-01-01 00:05:00\", \"INVALID\", \"1000.0\"),  # Bad: invalid region\n    (\"2024-01-01 00:05:00\", \"SA1\", \"-500\"),  # Bad: negative demand\n    (None, \"QLD1\", \"6000.0\"),  # Good (null timestamp is allowed)\n    (\"2024-01-01 00:10:00\", None, \"3000.0\"),  # Bad: null region\n]\n\ntest_df = spark.createDataFrame(test_data, [\"SETTLEMENTDATE\", \"REGIONID\", \"TOTALDEMAND\"])\n\nprint(\"Processing test data...\")\ngood_df = process_with_dlq(test_df, dlq_table)\n\nprint(\"\\nGood records:\")\ngood_df.show()\n\nprint(\"\\nDLQ contents:\")\nspark.read.table(dlq_table).show(truncate=False)\n",
   "metadata": {},
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "",
   "metadata": {},
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "## Complete Fault-Tolerant Pipeline\n",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "\ndef run_fault_tolerant_pipeline(\n    checkpoint_table: str,\n    dlq_table: str,\n    target_table: str,\n    regions: list,\n    start_date: str,\n    end_date: str,\n    use_sample: bool = True\n):\n    \"\"\"\n    Production-ready pipeline with all fault tolerance patterns.\n\n    SOLUTION: Complete implementation\n    \"\"\"\n    print(\"=\" * 60)\n    print(\"FAULT-TOLERANT NEMWEB PIPELINE\")\n    print(\"=\" * 60)\n\n    # Step 1: Check for completed partitions\n    print(\"\\n1. Checking checkpoint status...\")\n    completed = get_completed_partitions(checkpoint_table)\n    print(f\"   Found {len(completed)} completed partitions\")\n\n    # Step 2: Read data (with retry built into data source)\n    print(\"\\n2. Reading data...\")\n    df = (spark.read\n          .format(\"nemweb\")\n          .option(\"table\", \"DISPATCHREGIONSUM\")\n          .option(\"regions\", \",\".join(regions))\n          .option(\"start_date\", start_date)\n          .option(\"end_date\", end_date)\n          .option(\"use_sample\", str(use_sample).lower())\n          .load())\n\n    initial_count = df.count()\n    print(f\"   Loaded {initial_count} rows\")\n\n    if initial_count == 0:\n        print(\"   No data to process\")\n        return\n\n    # Step 3: Validate and route bad records to DLQ\n    print(\"\\n3. Validating data...\")\n    good_df = process_with_dlq(df, dlq_table)\n\n    # Step 4: Write to target\n    good_count = good_df.count()\n    if good_count > 0:\n        print(f\"\\n4. Writing {good_count} rows to {target_table}...\")\n        (good_df.write\n            .format(\"delta\")\n            .mode(\"append\")\n            .option(\"mergeSchema\", \"true\")\n            .saveAsTable(target_table))\n\n    # Step 5: Update checkpoints\n    print(\"\\n5. Updating checkpoints...\")\n    for region in regions:\n        id_string = f\"DISPATCHREGIONSUM:{region}:{start_date}:{end_date}\"\n        partition_id = hashlib.md5(id_string.encode()).hexdigest()[:12]\n\n        if partition_id not in completed:\n            mark_partition_complete(\n                checkpoint_table, partition_id, region,\n                start_date, end_date, good_count // len(regions)\n            )\n\n    print(\"\\n\" + \"=\" * 60)\n    print(\"PIPELINE COMPLETE\")\n    print(f\"  - Processed: {initial_count} rows\")\n    print(f\"  - Valid: {good_count} rows\")\n    print(f\"  - To DLQ: {initial_count - good_count} rows\")\n    print(\"=\" * 60)\n\n\n# Demo run\n# run_fault_tolerant_pipeline(\n#     checkpoint_table=\"nemweb_checkpoints_solution\",\n#     dlq_table=\"nemweb_dlq_solution\",\n#     target_table=\"nemweb_bronze_solution\",\n#     regions=[\"NSW1\", \"VIC1\"],\n#     start_date=\"2024-01-01\",\n#     end_date=\"2024-01-07\",\n#     use_sample=True\n# )\n",
   "metadata": {},
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "",
   "metadata": {},
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "## Key Takeaways\n1. **Retry with backoff**: Handles transient failures automatically\n2. **Checkpoint tables**: Enable batch job resume after failures\n3. **Streaming offsets**: Spark manages recovery automatically\n4. **Dead letter queues**: Preserve bad records for investigation\n5. **Idempotency**: Design operations that can safely re-run\n",
   "metadata": {}
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "dependencies": [
     "/Workspace/Users/david.okeeffe@databricks.com/.bundle/nemweb-lab/dev/files/databricks-nemweb-lab/artifacts/nemweb_datasource-2.10.7-py3-none-any.whl"
    ],
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "04_fault_tolerance_solution",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
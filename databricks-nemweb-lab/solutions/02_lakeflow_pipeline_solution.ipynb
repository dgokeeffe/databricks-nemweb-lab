{
 "cells": [
  {
   "cell_type": "markdown",
   "source": "# Solution: Exercise 2 - Lakeflow Pipeline Integration\nThis notebook contains the complete solutions for Exercise 2.\n",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "\n# New syntax: pyspark.pipelines replaces dlt (DBR 15.4+/Spark 4.0+)\nfrom pyspark import pipelines as dp\nfrom pyspark.sql.functions import col, current_timestamp, expr, avg, max, min, count, sum, when\n\n# Import spark from Databricks SDK for IDE support and local development\nfrom databricks.sdk.runtime import spark\n\n# Import and register our production Arrow data source\nfrom nemweb_datasource_arrow import NemwebArrowDataSource\nspark.dataSource.register(NemwebArrowDataSource)\n",
   "metadata": {},
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "",
   "metadata": {},
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "## Solution 2.1: Bronze Layer - Raw Ingestion\n",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "\n@dp.table(\n    name=\"nemweb_bronze\",\n    comment=\"Raw NEMWEB dispatch region data from custom data source\",\n    table_properties={\n        \"quality\": \"bronze\",\n        \"source\": \"nemweb.com.au\"\n    }\n)\ndef nemweb_bronze():\n    \"\"\"\n    Ingest raw NEMWEB data using production Arrow data source.\n\n    SOLUTION 2.1: Complete bronze table\n    \"\"\"\n    return (\n        spark.read\n        .format(\"nemweb_arrow\")  # Production Arrow data source\n        .option(\"table\", \"DISPATCHREGIONSUM\")  # Demand/generation data\n        .option(\"regions\", \"NSW1,QLD1,SA1,VIC1,TAS1\")  # All 5 NEM regions\n        .option(\"start_date\", spark.conf.get(\"nemweb.start_date\", \"2024-01-01\"))\n        .option(\"end_date\", spark.conf.get(\"nemweb.end_date\", \"2024-01-31\"))\n        .load()\n        .withColumn(\"_ingested_at\", current_timestamp())  # Add metadata\n    )\n",
   "metadata": {},
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "",
   "metadata": {},
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "## Solution 2.2: Silver Layer with Data Quality\n",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "\n@dp.table(\n    name=\"nemweb_silver\",\n    comment=\"Cleansed NEMWEB data with quality checks\"\n)\n@dp.expect_or_drop(\"valid_region\", \"region_id IN ('NSW1', 'QLD1', 'SA1', 'VIC1', 'TAS1')\")\n@dp.expect_or_drop(\"valid_demand\", \"total_demand_mw > 0\")  # SOLUTION 2.2a\n@dp.expect_or_drop(\"valid_timestamp\", \"settlement_date IS NOT NULL\")  # SOLUTION 2.2b\n@dp.expect_or_drop(\"reasonable_demand\", \"total_demand_mw < 20000\")  # SOLUTION 2.2c\ndef nemweb_silver():\n    \"\"\"\n    Cleanse and validate NEMWEB data.\n\n    Expectations:\n    - valid_region: Only accept known NEM regions\n    - valid_demand: Demand must be positive\n    - valid_timestamp: Must have settlement date\n    - reasonable_demand: Demand under 20,000 MW (Australia's total capacity ~60GW)\n    \"\"\"\n    return (\n        dp.read(\"nemweb_bronze\")\n        .select(\n            col(\"SETTLEMENTDATE\").cast(\"timestamp\").alias(\"settlement_date\"),\n            col(\"REGIONID\").alias(\"region_id\"),\n            col(\"TOTALDEMAND\").cast(\"double\").alias(\"total_demand_mw\"),\n            col(\"AVAILABLEGENERATION\").cast(\"double\").alias(\"available_generation_mw\"),\n            col(\"NETINTERCHANGE\").cast(\"double\").alias(\"net_interchange_mw\"),\n            col(\"DISPATCHINTERVAL\").alias(\"dispatch_interval\"),\n            col(\"_ingested_at\")\n        )\n        .withColumn(\"demand_generation_ratio\",\n                    when(col(\"available_generation_mw\") > 0,\n                         col(\"total_demand_mw\") / col(\"available_generation_mw\")))\n    )\n",
   "metadata": {},
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "",
   "metadata": {},
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "## Solution 2.3: Gold Layer - Hourly Aggregations\n",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "\n@dp.table(\n    name=\"nemweb_gold_hourly\",\n    comment=\"Hourly regional demand aggregations\"\n)\ndef nemweb_gold_hourly():\n    \"\"\"\n    Aggregate demand metrics by hour and region.\n\n    SOLUTION 2.3: Complete aggregations\n    \"\"\"\n    return (\n        dp.read(\"nemweb_silver\")\n        .withColumn(\"hour\", expr(\"date_trunc('hour', settlement_date)\"))\n        .groupBy(\"region_id\", \"hour\")\n        .agg(\n            avg(\"total_demand_mw\").alias(\"avg_demand_mw\"),\n            max(\"total_demand_mw\").alias(\"max_demand_mw\"),\n            min(\"total_demand_mw\").alias(\"min_demand_mw\"),\n            count(\"*\").alias(\"interval_count\"),\n            avg(\"demand_generation_ratio\").alias(\"avg_demand_gen_ratio\")\n        )\n    )\n",
   "metadata": {},
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "",
   "metadata": {},
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "## Additional Gold Tables\n",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "\n@dp.table(\n    name=\"nemweb_gold_daily\",\n    comment=\"Daily regional summary\"\n)\ndef nemweb_gold_daily():\n    \"\"\"Daily aggregations for regional analysis.\"\"\"\n    return (\n        dp.read(\"nemweb_silver\")\n        .withColumn(\"date\", expr(\"date(settlement_date)\"))\n        .groupBy(\"region_id\", \"date\")\n        .agg(\n            avg(\"total_demand_mw\").alias(\"avg_demand_mw\"),\n            max(\"total_demand_mw\").alias(\"peak_demand_mw\"),\n            min(\"total_demand_mw\").alias(\"min_demand_mw\"),\n            avg(\"net_interchange_mw\").alias(\"avg_net_interchange_mw\"),\n            count(\"*\").alias(\"interval_count\")\n        )\n        .withColumn(\"is_net_exporter\", col(\"avg_net_interchange_mw\") < 0)\n    )\n\n\n@dp.table(\n    name=\"nemweb_gold_nem_totals\",\n    comment=\"NEM-wide totals across all regions\"\n)\ndef nemweb_gold_nem_totals():\n    \"\"\"Total NEM demand by hour.\"\"\"\n    return (\n        dp.read(\"nemweb_silver\")\n        .withColumn(\"hour\", expr(\"date_trunc('hour', settlement_date)\"))\n        .groupBy(\"hour\")\n        .agg(\n            sum(\"total_demand_mw\").alias(\"total_nem_demand_mw\"),\n            sum(\"available_generation_mw\").alias(\"total_available_generation_mw\"),\n            count(\"*\").alias(\"region_intervals\")\n        )\n    )\n",
   "metadata": {},
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "",
   "metadata": {},
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "## Validation (Non-DLT Test)\n",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "\ndef validate_pipeline_logic():\n    \"\"\"Validate pipeline logic without running actual DLT.\"\"\"\n\n    # Create test data\n    test_data = [\n        (\"2024-01-01 00:05:00\", \"NSW1\", \"7500.5\", \"8000.0\", \"-200.5\", \"1\"),\n        (\"2024-01-01 00:10:00\", \"VIC1\", \"5200.3\", \"5500.0\", \"150.2\", \"2\"),\n        (\"2024-01-01 00:10:00\", \"INVALID\", \"-100\", \"0\", \"0\", \"2\"),  # Invalid region\n        (\"2024-01-01 00:15:00\", \"SA1\", \"-500\", \"2000.0\", \"100.0\", \"3\"),  # Negative demand\n        (None, \"QLD1\", \"6000.0\", \"6500.0\", \"-50.0\", \"1\"),  # Null timestamp\n    ]\n\n    df = spark.createDataFrame(test_data, [\n        \"SETTLEMENTDATE\", \"REGIONID\", \"TOTALDEMAND\",\n        \"AVAILABLEGENERATION\", \"NETINTERCHANGE\", \"DISPATCHINTERVAL\"\n    ])\n\n    print(f\"Input rows: {df.count()}\")\n\n    # Apply quality checks (simulating expectations)\n    valid_regions = [\"NSW1\", \"QLD1\", \"SA1\", \"VIC1\", \"TAS1\"]\n\n    silver_df = (df\n        .filter(col(\"REGIONID\").isin(valid_regions))  # valid_region\n        .filter(col(\"TOTALDEMAND\").cast(\"double\") > 0)  # valid_demand\n        .filter(col(\"SETTLEMENTDATE\").isNotNull())  # valid_timestamp\n        .filter(col(\"TOTALDEMAND\").cast(\"double\") < 20000)  # reasonable_demand\n    )\n\n    print(f\"After quality checks: {silver_df.count()} rows\")\n    print(f\"Dropped: {df.count() - silver_df.count()} rows\")\n\n    # Test aggregation\n    gold_df = (silver_df\n        .select(\n            col(\"REGIONID\").alias(\"region_id\"),\n            col(\"TOTALDEMAND\").cast(\"double\").alias(\"total_demand_mw\")\n        )\n        .groupBy(\"region_id\")\n        .agg(\n            avg(\"total_demand_mw\").alias(\"avg_demand\"),\n            max(\"total_demand_mw\").alias(\"max_demand\"),\n            count(\"*\").alias(\"count\")\n        )\n    )\n\n    print(\"\\nAggregated results:\")\n    gold_df.show()\n\n    print(\"\\nPipeline logic validated!\")\n    return True\n\nvalidate_pipeline_logic()\n",
   "metadata": {},
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "",
   "metadata": {},
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "## Pipeline Configuration Example\n",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "\npipeline_config = {\n    \"name\": \"nemweb-ingestion-pipeline\",\n    \"target\": \"nemweb_data\",\n    \"catalog\": \"workspace\",  # Change to your catalog\n    \"development\": False,\n    \"continuous\": False,\n    \"channel\": \"CURRENT\",\n    \"photon\": True,\n    \"libraries\": [\n        {\"notebook\": {\"path\": \"/path/to/02_lakeflow_pipeline_solution\"}}\n    ],\n    \"configuration\": {\n        \"nemweb.start_date\": \"2024-01-01\",\n        \"nemweb.end_date\": \"2024-06-30\",\n        \"spark.sql.shuffle.partitions\": \"auto\"\n    }\n}\n\nimport json\nprint(\"Pipeline Configuration:\")\nprint(json.dumps(pipeline_config, indent=2))\n",
   "metadata": {},
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "dependencies": [
     "/Workspace/Users/david.okeeffe@databricks.com/.bundle/nemweb-lab/dev/files/databricks-nemweb-lab/artifacts/nemweb_datasource-2.10.7-py3-none-any.whl"
    ],
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "02_lakeflow_pipeline_solution",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
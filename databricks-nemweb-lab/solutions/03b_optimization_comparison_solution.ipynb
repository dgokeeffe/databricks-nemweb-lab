{
 "cells": [
  {
   "cell_type": "markdown",
   "source": "# Solution: Exercise 3 - Delta Lake Optimization Techniques\n**Time:** 15 minutes\nThis is the **final main exercise** of the lab. This solution demonstrates Delta Lake\noptimization techniques critical for production workloads on Databricks.\n## Topics Covered\n1. **Liquid Clustering** - Modern data layout (DBR 13.3+)\n2. **Generated Columns + Partitioning** - Traditional approach\n3. **OPTIMIZE** - File compaction and data layout\n4. **VACUUM** - Storage cleanup\n5. **Predictive Optimization** - Automatic maintenance\n## Learning Objectives\n1. Understand when to use liquid clustering vs. partitioning\n2. Measure query performance differences (files scanned, bytes read)\n3. Configure Delta table maintenance for production\n## Prerequisites\n- Run **00_setup_and_validation.py** first to pre-load NEMWEB data\n",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "",
   "metadata": {},
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "## Configuration\nParameters should match those used in 00_setup_and_validation.py\n",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "\nfrom databricks.sdk.runtime import spark, display\n\n# Create widgets with defaults matching setup notebook\ndbutils.widgets.text(\"catalog\", \"workspace\", \"Catalog Name\")\ndbutils.widgets.text(\"schema\", \"nemweb_lab\", \"Source Schema\")\ndbutils.widgets.text(\"table\", \"nemweb_raw\", \"Source Table\")\ndbutils.widgets.text(\"target_schema\", \"nemweb_optimization_lab\", \"Target Schema\")\n\n# Get configuration from widgets\nSOURCE_CATALOG = dbutils.widgets.get(\"catalog\")\nSOURCE_SCHEMA = dbutils.widgets.get(\"schema\")\nSOURCE_TABLE = dbutils.widgets.get(\"table\")\nTARGET_SCHEMA = dbutils.widgets.get(\"target_schema\")\n\nprint(\"Configuration\")\nprint(\"=\" * 50)\nprint(f\"Catalog:       {SOURCE_CATALOG}\")\nprint(f\"Source Schema: {SOURCE_SCHEMA}\")\nprint(f\"Source Table:  {SOURCE_TABLE}\")\nprint(f\"Target Schema: {TARGET_SCHEMA}\")\n\n# Verify source table exists\nsource_table_path = f\"{SOURCE_CATALOG}.{SOURCE_SCHEMA}.{SOURCE_TABLE}\"\nif not spark.catalog.tableExists(source_table_path):\n    raise RuntimeError(\n        f\"Source table {source_table_path} not found!\\n\"\n        \"Please run 00_setup_and_validation.py first to pre-load data.\"\n    )\n\n# Create target schema\nspark.sql(f\"CREATE SCHEMA IF NOT EXISTS {SOURCE_CATALOG}.{TARGET_SCHEMA}\")\nspark.sql(f\"USE {SOURCE_CATALOG}.{TARGET_SCHEMA}\")\n\nrow_count = spark.table(source_table_path).count()\nprint(f\"✓ Source table: {source_table_path}\")\nprint(f\"✓ Row count: {row_count:,}\")\nprint(f\"✓ Target schema: {SOURCE_CATALOG}.{TARGET_SCHEMA}\")\n",
   "metadata": {},
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "",
   "metadata": {},
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "## Load Pre-loaded NEMWEB Data\n",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "\nfrom pyspark.sql.functions import col, current_timestamp, when\n\n# Read from pre-loaded table\nnemweb_raw = spark.table(source_table_path)\n\n# Prepare data with standardized column names\nnemweb_data = (\n    nemweb_raw\n    .select(\n        col(\"SETTLEMENTDATE\").cast(\"timestamp\").alias(\"settlement_date\"),\n        col(\"REGIONID\").alias(\"region_id\"),\n        col(\"TOTALDEMAND\").cast(\"double\").alias(\"total_demand_mw\"),\n        col(\"AVAILABLEGENERATION\").cast(\"double\").alias(\"available_generation_mw\"),\n        col(\"NETINTERCHANGE\").cast(\"double\").alias(\"net_interchange_mw\")\n    )\n    .withColumn(\"_loaded_at\", current_timestamp())\n    # Generate a synthetic RRP for price spike queries (real RRP requires DISPATCHPRICE table)\n    .withColumn(\"rrp\",\n        when(col(\"total_demand_mw\") > 8000, 300 + (col(\"total_demand_mw\") - 8000) * 0.5)\n        .otherwise(30 + col(\"total_demand_mw\") * 0.01)\n    )\n)\n\n# Cache for reuse\nnemweb_data.cache()\nprint(f\"Prepared {nemweb_data.count():,} rows for optimization comparison\")\nnemweb_data.show(5)\n",
   "metadata": {},
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "",
   "metadata": {},
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "## Approach 1: Liquid Clustering\nCluster by the columns most commonly used in query filters.\n",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "\n# Drop if exists for clean comparison\nspark.sql(\"DROP TABLE IF EXISTS nemweb_liquid_clustered\")\n\n# Create table with liquid clustering on settlement_date and region_id\nnemweb_data.write \\\n    .format(\"delta\") \\\n    .option(\"delta.enableChangeDataFeed\", \"false\") \\\n    .clusterBy(\"settlement_date\", \"region_id\") \\\n    .saveAsTable(\"nemweb_liquid_clustered\")\n\nprint(\"✓ Created: nemweb_liquid_clustered\")\nprint(\"  Clustering keys: settlement_date, region_id\")\n",
   "metadata": {},
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "\n# Run OPTIMIZE to apply clustering\nspark.sql(\"OPTIMIZE nemweb_liquid_clustered\")\nprint(\"✓ OPTIMIZE complete - data is now clustered\")\n",
   "metadata": {},
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "",
   "metadata": {},
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "## Approach 2: Generated Columns + Partitioning\nTraditional approach: generate date columns and partition by them.\n",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "\n# Drop if exists\nspark.sql(\"DROP TABLE IF EXISTS nemweb_partitioned\")\n\n# Create table with generated columns for partitioning\nspark.sql(\"\"\"\nCREATE TABLE nemweb_partitioned (\n    settlement_date TIMESTAMP,\n    region_id STRING,\n    total_demand_mw DOUBLE,\n    available_generation_mw DOUBLE,\n    net_interchange_mw DOUBLE,\n    rrp DOUBLE,\n    _loaded_at TIMESTAMP,\n    -- Generated columns for partition pruning\n    settlement_year INT GENERATED ALWAYS AS (YEAR(settlement_date)),\n    settlement_month INT GENERATED ALWAYS AS (MONTH(settlement_date)),\n    settlement_day INT GENERATED ALWAYS AS (DAY(settlement_date))\n)\nPARTITIONED BY (settlement_year, settlement_month)\n\"\"\")\n\nprint(\"✓ Created: nemweb_partitioned\")\nprint(\"  Partitioned by: settlement_year, settlement_month\")\nprint(\"  Generated columns: settlement_year, settlement_month, settlement_day\")\n",
   "metadata": {},
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "\n# Insert data (generated columns are auto-computed)\nnemweb_data.select(\n    \"settlement_date\", \"region_id\", \"total_demand_mw\",\n    \"available_generation_mw\", \"net_interchange_mw\", \"rrp\", \"_loaded_at\"\n).write.mode(\"append\").insertInto(\"nemweb_partitioned\")\n\nprint(\"✓ Data inserted - generated columns computed automatically\")\n\n# Optimize each partition\nspark.sql(\"OPTIMIZE nemweb_partitioned\")\nprint(\"✓ OPTIMIZE complete\")\n",
   "metadata": {},
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "",
   "metadata": {},
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "## Performance Comparison\nNow let's run identical queries against both tables and compare metrics.\n",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "",
   "metadata": {},
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "### Helper Function: Capture Query Metrics\n",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "\nimport time\n\ndef run_query_with_metrics(query: str, description: str) -> dict:\n    \"\"\"Run a query and capture performance metrics.\"\"\"\n    # Clear cache to ensure fair comparison\n    spark.catalog.clearCache()\n\n    # Execute and time\n    start = time.time()\n    df = spark.sql(query)\n    result = df.collect()  # Force execution\n    elapsed = time.time() - start\n\n    return {\n        \"description\": description,\n        \"elapsed_seconds\": round(elapsed, 3),\n        \"row_count\": len(result)\n    }\n",
   "metadata": {},
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "",
   "metadata": {},
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "### Query 1: Single Day Filter (Narrow Time Range)\nFilter for a specific day - tests partition/cluster pruning effectiveness.\n",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "\n# Use a date in the middle of the data range\nquery_liquid = \"\"\"\nSELECT region_id,\n       AVG(total_demand_mw) as avg_demand,\n       MAX(rrp) as max_price,\n       COUNT(*) as intervals\nFROM nemweb_liquid_clustered\nWHERE settlement_date >= '2024-03-15'\n  AND settlement_date < '2024-03-16'\nGROUP BY region_id\nORDER BY region_id\n\"\"\"\n\nquery_partitioned = \"\"\"\nSELECT region_id,\n       AVG(total_demand_mw) as avg_demand,\n       MAX(rrp) as max_price,\n       COUNT(*) as intervals\nFROM nemweb_partitioned\nWHERE settlement_date >= '2024-03-15'\n  AND settlement_date < '2024-03-16'\nGROUP BY region_id\nORDER BY region_id\n\"\"\"\n\nprint(\"=\" * 60)\nprint(\"QUERY 1: Single Day Aggregation\")\nprint(\"=\" * 60)\n\nresult_liquid = run_query_with_metrics(query_liquid, \"Liquid Clustered\")\nresult_partitioned = run_query_with_metrics(query_partitioned, \"Partitioned + Generated\")\n\nprint(f\"\\nLiquid Clustered:        {result_liquid['elapsed_seconds']:.3f}s ({result_liquid['row_count']} rows)\")\nprint(f\"Partitioned + Generated: {result_partitioned['elapsed_seconds']:.3f}s ({result_partitioned['row_count']} rows)\")\n",
   "metadata": {},
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "",
   "metadata": {},
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "### Query 2: Single Region, Full History\nFilter for one region across all time - tests region-based filtering.\n",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "\nquery_liquid_2 = \"\"\"\nSELECT DATE(settlement_date) as date,\n       AVG(total_demand_mw) as avg_demand,\n       AVG(rrp) as avg_price\nFROM nemweb_liquid_clustered\nWHERE region_id = 'NSW1'\nGROUP BY DATE(settlement_date)\nORDER BY date\n\"\"\"\n\nquery_partitioned_2 = \"\"\"\nSELECT DATE(settlement_date) as date,\n       AVG(total_demand_mw) as avg_demand,\n       AVG(rrp) as avg_price\nFROM nemweb_partitioned\nWHERE region_id = 'NSW1'\nGROUP BY DATE(settlement_date)\nORDER BY date\n\"\"\"\n\nprint(\"=\" * 60)\nprint(\"QUERY 2: Single Region Full History\")\nprint(\"=\" * 60)\n\nresult_liquid_2 = run_query_with_metrics(query_liquid_2, \"Liquid Clustered\")\nresult_partitioned_2 = run_query_with_metrics(query_partitioned_2, \"Partitioned + Generated\")\n\nprint(f\"\\nLiquid Clustered:        {result_liquid_2['elapsed_seconds']:.3f}s ({result_liquid_2['row_count']} rows)\")\nprint(f\"Partitioned + Generated: {result_partitioned_2['elapsed_seconds']:.3f}s ({result_partitioned_2['row_count']} rows)\")\n",
   "metadata": {},
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "",
   "metadata": {},
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "### Query 3: Month Range + Region Filter (Combined)\nFilter by both time range and region - tests combined pruning.\n",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "\nquery_liquid_3 = \"\"\"\nSELECT region_id,\n       HOUR(settlement_date) as hour_of_day,\n       AVG(total_demand_mw) as avg_demand,\n       PERCENTILE_APPROX(rrp, 0.95) as p95_price\nFROM nemweb_liquid_clustered\nWHERE settlement_date >= '2024-02-01'\n  AND settlement_date < '2024-04-01'\n  AND region_id IN ('NSW1', 'VIC1')\nGROUP BY region_id, HOUR(settlement_date)\nORDER BY region_id, hour_of_day\n\"\"\"\n\nquery_partitioned_3 = \"\"\"\nSELECT region_id,\n       HOUR(settlement_date) as hour_of_day,\n       AVG(total_demand_mw) as avg_demand,\n       PERCENTILE_APPROX(rrp, 0.95) as p95_price\nFROM nemweb_partitioned\nWHERE settlement_date >= '2024-02-01'\n  AND settlement_date < '2024-04-01'\n  AND region_id IN ('NSW1', 'VIC1')\nGROUP BY region_id, HOUR(settlement_date)\nORDER BY region_id, hour_of_day\n\"\"\"\n\nprint(\"=\" * 60)\nprint(\"QUERY 3: Month Range + Region Filter\")\nprint(\"=\" * 60)\n\nresult_liquid_3 = run_query_with_metrics(query_liquid_3, \"Liquid Clustered\")\nresult_partitioned_3 = run_query_with_metrics(query_partitioned_3, \"Partitioned + Generated\")\n\nprint(f\"\\nLiquid Clustered:        {result_liquid_3['elapsed_seconds']:.3f}s ({result_liquid_3['row_count']} rows)\")\nprint(f\"Partitioned + Generated: {result_partitioned_3['elapsed_seconds']:.3f}s ({result_partitioned_3['row_count']} rows)\")\n",
   "metadata": {},
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "",
   "metadata": {},
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "### Query 4: Price Spike Detection (No Time/Region Filter)\nScan for price spikes - tests full table scan performance.\n",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "\nquery_liquid_4 = \"\"\"\nSELECT settlement_date, region_id, rrp, total_demand_mw\nFROM nemweb_liquid_clustered\nWHERE rrp > 300\nORDER BY rrp DESC\nLIMIT 100\n\"\"\"\n\nquery_partitioned_4 = \"\"\"\nSELECT settlement_date, region_id, rrp, total_demand_mw\nFROM nemweb_partitioned\nWHERE rrp > 300\nORDER BY rrp DESC\nLIMIT 100\n\"\"\"\n\nprint(\"=\" * 60)\nprint(\"QUERY 4: Price Spike Detection (Full Scan)\")\nprint(\"=\" * 60)\n\nresult_liquid_4 = run_query_with_metrics(query_liquid_4, \"Liquid Clustered\")\nresult_partitioned_4 = run_query_with_metrics(query_partitioned_4, \"Partitioned + Generated\")\n\nprint(f\"\\nLiquid Clustered:        {result_liquid_4['elapsed_seconds']:.3f}s ({result_liquid_4['row_count']} rows)\")\nprint(f\"Partitioned + Generated: {result_partitioned_4['elapsed_seconds']:.3f}s ({result_partitioned_4['row_count']} rows)\")\n",
   "metadata": {},
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "",
   "metadata": {},
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "## Results Summary\n",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "\nfrom pyspark.sql import Row\n\nsummary_data = [\n    Row(query=\"Q1: Single Day\", liquid=result_liquid['elapsed_seconds'], partitioned=result_partitioned['elapsed_seconds']),\n    Row(query=\"Q2: Single Region History\", liquid=result_liquid_2['elapsed_seconds'], partitioned=result_partitioned_2['elapsed_seconds']),\n    Row(query=\"Q3: Month + Region\", liquid=result_liquid_3['elapsed_seconds'], partitioned=result_partitioned_3['elapsed_seconds']),\n    Row(query=\"Q4: Price Spikes (Full)\", liquid=result_liquid_4['elapsed_seconds'], partitioned=result_partitioned_4['elapsed_seconds']),\n]\n\nsummary_df = spark.createDataFrame(summary_data)\nsummary_df = summary_df.withColumn(\"winner\",\n    when(col(\"liquid\") < col(\"partitioned\"), \"Liquid Clustering\")\n    .when(col(\"partitioned\") < col(\"liquid\"), \"Partitioned\")\n    .otherwise(\"Tie\")\n)\n\nprint(\"\\n\" + \"=\" * 60)\nprint(\"PERFORMANCE SUMMARY (seconds)\")\nprint(\"=\" * 60)\nsummary_df.show(truncate=False)\n",
   "metadata": {},
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "",
   "metadata": {},
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "## Table Statistics Comparison\n",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "",
   "metadata": {},
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "%sql",
   "metadata": {},
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "-- Liquid clustered table details",
   "metadata": {},
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "DESCRIBE DETAIL nemweb_liquid_clustered\n",
   "metadata": {},
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "",
   "metadata": {},
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "%sql",
   "metadata": {},
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "-- Partitioned table details",
   "metadata": {},
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "DESCRIBE DETAIL nemweb_partitioned\n",
   "metadata": {},
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "\n# Get file counts and sizes\nliquid_detail = spark.sql(\"DESCRIBE DETAIL nemweb_liquid_clustered\").collect()[0]\npartitioned_detail = spark.sql(\"DESCRIBE DETAIL nemweb_partitioned\").collect()[0]\n\nprint(\"\\nTable Statistics:\")\nprint(\"-\" * 50)\nprint(f\"{'Metric':<25} {'Liquid':<15} {'Partitioned':<15}\")\nprint(\"-\" * 50)\nprint(f\"{'Num Files':<25} {liquid_detail['numFiles']:<15} {partitioned_detail['numFiles']:<15}\")\nprint(f\"{'Size (MB)':<25} {liquid_detail['sizeInBytes']/1024/1024:<15.2f} {partitioned_detail['sizeInBytes']/1024/1024:<15.2f}\")\n",
   "metadata": {},
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "",
   "metadata": {},
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "## Part 5: Delta Table Maintenance\n### OPTIMIZE - File Compaction\nThe `OPTIMIZE` command compacts small files and applies clustering/Z-ORDER.\n```sql\n-- Basic OPTIMIZE (compacts files, applies clustering if defined)\nOPTIMIZE nemweb_liquid_clustered\n-- OPTIMIZE with Z-ORDER (legacy alternative to liquid clustering)\nOPTIMIZE nemweb_legacy ZORDER BY (settlement_date, region_id)\n```\n**When to use:**\n- After batch ingestion jobs\n- When you see many small files (check with DESCRIBE DETAIL)\n- For liquid clustered tables: data gets better organized with each OPTIMIZE\n### VACUUM - Storage Cleanup\nThe `VACUUM` command removes old files that are no longer referenced.\n```sql\n-- Remove files older than 7 days (default retention)\nVACUUM nemweb_liquid_clustered\n-- Remove files older than 24 hours (requires safety check override)\nVACUUM nemweb_liquid_clustered RETAIN 24 HOURS\n```\n**Important:** VACUUM deletes time travel history! Set retention carefully.\n### Predictive Optimization (Unity Catalog)\nFor managed tables in Unity Catalog, enable automatic OPTIMIZE and VACUUM:\n```sql\n-- Enable at catalog level\nALTER CATALOG my_catalog SET (\n'predictiveOptimization' = 'ENABLE'\n)\n-- Or enable per table\nALTER TABLE nemweb_bronze SET TBLPROPERTIES (\n'delta.enablePredictiveOptimization' = 'true'\n)\n```\n**Benefits:**\n- Automatic file compaction when needed\n- No manual OPTIMIZE scheduling required\n- Intelligent scheduling based on table activity\n",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "",
   "metadata": {},
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "## Key Takeaways\n| Scenario | Recommended Approach | Why |\n|----------|---------------------|-----|\n| New tables | **Liquid Clustering** | Simpler, flexible, modern |\n| Time-series with varied queries | **Liquid Clustering** | Handles both time and dimension filters |\n| Very high cardinality partition keys | **Liquid Clustering** | Avoids small file problem |\n| Strict partition pruning needed | **Generated + Partitioning** | Explicit partition boundaries |\n| Existing partitioned tables | Keep partitioning OR migrate | Migration requires rewrite |\n| Production maintenance | **Predictive Optimization** | Automatic OPTIMIZE/VACUUM |\n### For NEMWEB Data\n1. **Liquid clustering** excels when queries filter on multiple dimensions (time + region)\n2. **Partitioning** is effective for pure time-range queries with known boundaries\n3. Both approaches benefit from running `OPTIMIZE` regularly\n4. On serverless, **data layout optimization is your primary performance lever**\n5. Enable **Predictive Optimization** for automatic maintenance in production\n### Delta Optimization Checklist\n- [x] Choose data layout strategy (liquid clustering preferred for new tables)\n- [x] Run OPTIMIZE after batch loads or enable predictive optimization\n- [x] Configure VACUUM retention based on time travel needs\n- [x] Monitor file count and sizes with DESCRIBE DETAIL\n",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "",
   "metadata": {},
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "## Cleanup\n",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "\n# Uncache source data\nnemweb_data.unpersist()\n\n# Uncomment to clean up test tables\n# spark.sql(\"DROP TABLE IF EXISTS nemweb_liquid_clustered\")\n# spark.sql(\"DROP TABLE IF EXISTS nemweb_partitioned\")\n# spark.sql(f\"DROP SCHEMA IF EXISTS {SOURCE_CATALOG}.{TARGET_SCHEMA}\")\n# print(\"Cleanup complete\")\n",
   "metadata": {},
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "dependencies": [
     "/Workspace/Users/david.okeeffe@databricks.com/.bundle/nemweb-lab/dev/files/databricks-nemweb-lab/artifacts/nemweb_datasource-2.10.7-py3-none-any.whl"
    ],
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "03b_optimization_comparison_solution",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
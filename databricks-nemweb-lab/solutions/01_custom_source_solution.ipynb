{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solution: Exercise 1 - Building a Custom PySpark Data Source\n",
    "\n",
    "**Time:** 15 minutes\n",
    "\n",
    "This notebook contains the complete solutions for Exercise 1.\n",
    "\n",
    "## Learning Objectives\n",
    "1. Understand the Python Data Source API architecture\n",
    "2. Implement schema definition for external data\n",
    "3. Create partition-aware data reading for parallelism\n",
    "4. Register and use your custom data source\n",
    "\n",
    "## Reference\n",
    "- [Python Data Source API Docs](https://docs.databricks.com/en/pyspark/datasources.html)\n",
    "- [Example Implementations (GitHub)](https://github.com/allisonwang-db/pyspark-data-sources)\n",
    "- Production implementation: `src/nemweb_datasource_arrow.py`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Warm-up: Hello World Data Source (2 minutes)\n",
    "\n",
    "Let's start with the **simplest possible** custom data source to understand the API.\n",
    "\n",
    "A custom data source needs just **3 components**:\n",
    "1. `name()` - The format string for `spark.read.format(\"name\")`\n",
    "2. `schema()` - What columns and types your data has\n",
    "3. `reader()` - Creates a reader that fetches the data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Insight\n",
    "\n",
    "That's it! A custom data source is:\n",
    "- **DataSource class**: Declares the name, schema, and creates a reader\n",
    "- **DataSourceReader class**: Has a `read()` method that yields PyArrow RecordBatch\n",
    "\n",
    "For Serverless/Spark Connect compatibility, the `read()` method must yield **PyArrow RecordBatch** objects (not Row objects or tuples)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pyspark.sql.datasource import DataSource, DataSourceReader, InputPartition\n",
    "from pyspark.sql.types import (\n",
    "    StructType, StructField, StringType, DoubleType, TimestampType\n",
    ")\n",
    "from typing import Iterator, Tuple\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "def get_dispatchregionsum_schema() -> StructType:\n",
    "    \"\"\"\n",
    "    Return the schema for DISPATCHREGIONSUM table.\n",
    "\n",
    "    Reference: MMS Electricity Data Model Report - DISPATCH package\n",
    "    \"\"\"\n",
    "    return StructType([\n",
    "        # Time and identification fields\n",
    "        StructField(\"SETTLEMENTDATE\", TimestampType(), True),\n",
    "        StructField(\"RUNNO\", StringType(), True),\n",
    "        StructField(\"REGIONID\", StringType(), True),\n",
    "        StructField(\"DISPATCHINTERVAL\", StringType(), True),\n",
    "        StructField(\"INTERVENTION\", StringType(), True),\n",
    "\n",
    "        # SOLUTION 1.1: Added measurement fields\n",
    "        StructField(\"TOTALDEMAND\", DoubleType(), True),\n",
    "        StructField(\"AVAILABLEGENERATION\", DoubleType(), True),\n",
    "        StructField(\"AVAILABLELOAD\", DoubleType(), True),\n",
    "        StructField(\"DEMANDFORECAST\", DoubleType(), True),\n",
    "        StructField(\"DISPATCHABLEGENERATION\", DoubleType(), True),\n",
    "        StructField(\"DISPATCHABLELOAD\", DoubleType(), True),\n",
    "        StructField(\"NETINTERCHANGE\", DoubleType(), True),\n",
    "    ])\n",
    "\n",
    "# Verify schema\n",
    "schema = get_dispatchregionsum_schema()\n",
    "print(f\"Schema has {len(schema.fields)} fields (expected: 12)\")\n",
    "for field in schema.fields:\n",
    "    print(f\"  - {field.name}: {field.dataType}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Partitioning and Data Reading\n",
    "\n",
    "Spark achieves parallelism by dividing work into **partitions**. Each partition can be processed independently on different cores/nodes.\n",
    "\n",
    "For NEMWEB, we'll create one partition per NEM region:\n",
    "- NSW1 (New South Wales)\n",
    "- QLD1 (Queensland)  \n",
    "- SA1 (South Australia)\n",
    "- VIC1 (Victoria)\n",
    "- TAS1 (Tasmania)\n",
    "\n",
    "### Helper Functions\n",
    "\n",
    "We provide two helper functions:\n",
    "- `fetch_nemweb_current()` - Fetches real data from AEMO NEMWEB\n",
    "- `parse_nemweb_to_arrow()` - Converts data to PyArrow RecordBatch (required for Serverless)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import helper functions\n",
    "import sys\n",
    "import os\n",
    "\n",
    "notebook_path = dbutils.notebook.entry_point.getDbutils().notebook().getContext().notebookPath().get()\n",
    "repo_root = str(os.path.dirname(os.path.dirname(notebook_path)))\n",
    "sys.path.insert(0, f\"/Workspace{repo_root}/src\")\n",
    "\n",
    "from nemweb_utils import fetch_nemweb_current, parse_nemweb_to_arrow, get_version\n",
    "\n",
    "# Output version for debugging\n",
    "print(f\"nemweb_utils version: {get_version()}\")\n",
    "\n",
    "# Quick test - these helpers handle HTTP fetching and Arrow conversion\n",
    "# This fetches REAL data from AEMO NEMWEB CURRENT folder (last ~7 days)\n",
    "test_data = fetch_nemweb_current(\n",
    "    table=\"DISPATCHREGIONSUM\",\n",
    "    region=\"NSW1\",\n",
    "    max_files=2,\n",
    "    use_sample=False,  # Fetch real current files from AEMO\n",
    "    debug=True  # Print debug info\n",
    ")\n",
    "print(f\"Helper function works! Got {len(test_data)} rows\")\n",
    "if test_data:\n",
    "    # Note: Real AEMO data has MANY more columns (~130+) than our 12-field schema\n",
    "    # parse_nemweb_to_arrow() will filter to only the fields in our schema\n",
    "    all_keys = list(test_data[0].keys())\n",
    "    print(f\"Raw CSV has {len(all_keys)} columns (AEMO includes many fields)\")\n",
    "    print(f\"Sample row keys (first 10): {all_keys[:10]}...\")\n",
    "    print(f\"Sample SETTLEMENTDATE: {test_data[0].get('SETTLEMENTDATE')}\")\n",
    "    \n",
    "    # Verify key fields exist in real data\n",
    "    required_fields = [\"SETTLEMENTDATE\", \"REGIONID\", \"RUNNO\", \"TOTALDEMAND\", \n",
    "                      \"AVAILABLEGENERATION\", \"NETINTERCHANGE\"]\n",
    "    missing = [f for f in required_fields if f not in all_keys]\n",
    "    if missing:\n",
    "        print(f\"‚ö†Ô∏è  WARNING: Missing fields in real data: {missing}\")\n",
    "    else:\n",
    "        print(f\"‚úÖ All required fields present in real data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NemwebPartition(InputPartition):\n",
    "    \"\"\"\n",
    "    Represents one partition of NEMWEB data.\n",
    "    Each partition handles one region's data.\n",
    "    \"\"\"\n",
    "    def __init__(self, region: str, start_date: str, end_date: str):\n",
    "        self.region = region\n",
    "        self.start_date = start_date\n",
    "        self.end_date = end_date\n",
    "\n",
    "\n",
    "class NemwebReader(DataSourceReader):\n",
    "    \"\"\"\n",
    "    Reader for NEMWEB data source.\n",
    "\n",
    "    The reader has two jobs:\n",
    "    1. partitions() - Plan the work (called on driver)\n",
    "    2. read() - Do the work (called on workers)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, schema: StructType, options: dict):\n",
    "        self.schema = schema\n",
    "        self.options = options\n",
    "        self.regions = options.get(\"regions\", \"NSW1,QLD1,SA1,VIC1,TAS1\").split(\",\")\n",
    "        yesterday = (datetime.now() - timedelta(days=2)).strftime(\"%Y-%m-%d\")\n",
    "        self.start_date = options.get(\"start_date\", yesterday)\n",
    "        self.end_date = options.get(\"end_date\", yesterday)\n",
    "\n",
    "    def partitions(self) -> list[InputPartition]:\n",
    "        \"\"\"\n",
    "        Plan partitions for parallel reading.\n",
    "\n",
    "        SOLUTION 1.2a: Create one partition per region\n",
    "        \"\"\"\n",
    "        partitions = []\n",
    "\n",
    "        for region in self.regions:\n",
    "            partition = NemwebPartition(\n",
    "                region=region.strip(),\n",
    "                start_date=self.start_date,\n",
    "                end_date=self.end_date\n",
    "            )\n",
    "            partitions.append(partition)\n",
    "\n",
    "        return partitions\n",
    "\n",
    "    def read(self, partition: NemwebPartition):\n",
    "        \"\"\"\n",
    "        Read data for a single partition (runs on workers).\n",
    "\n",
    "        SOLUTION 1.2b: Fetch data and yield PyArrow RecordBatch\n",
    "        \"\"\"\n",
    "        # Fetch data - fetches REAL current files from AEMO NEMWEB\n",
    "        data = fetch_nemweb_current(\n",
    "            table=\"DISPATCHREGIONSUM\",\n",
    "            region=partition.region,\n",
    "            max_files=6,\n",
    "            use_sample=False,  # Fetch real current files from AEMO\n",
    "            debug=True  # Print debug info\n",
    "        )\n",
    "        \n",
    "        print(f\"[DEBUG] Fetched {len(data)} rows for region {partition.region}\")\n",
    "\n",
    "        # Convert to PyArrow RecordBatch (required for Serverless)\n",
    "        if data:\n",
    "            yield parse_nemweb_to_arrow(data, self.schema)\n",
    "\n",
    "\n",
    "# Test partition planning\n",
    "test_options = {\"regions\": \"NSW1,VIC1,QLD1\"}\n",
    "reader = NemwebReader(schema, test_options)\n",
    "partitions = reader.partitions()\n",
    "\n",
    "print(f\"Created {len(partitions)} partitions (expected: 3)\")\n",
    "for p in partitions:\n",
    "    print(f\"  - Region: {p.region}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NemwebDataSource(DataSource):\n",
    "    \"\"\"\n",
    "    Custom PySpark Data Source for AEMO NEMWEB electricity market data.\n",
    "\n",
    "    Usage:\n",
    "        spark.dataSource.register(NemwebDataSource)\n",
    "        df = spark.read.format(\"nemweb\").option(\"regions\", \"NSW1,VIC1\").load()\n",
    "\n",
    "    Options:\n",
    "        - regions: Comma-separated list of NEM regions (default: all 5)\n",
    "        - start_date: Start date in YYYY-MM-DD format\n",
    "        - end_date: End date in YYYY-MM-DD format\n",
    "    \"\"\"\n",
    "\n",
    "    @classmethod\n",
    "    def name(cls) -> str:\n",
    "        \"\"\"Return the format name used in spark.read.format(\"...\").\"\"\"\n",
    "        # SOLUTION 1.3a: Return the format name\n",
    "        return \"nemweb\"\n",
    "\n",
    "    def schema(self) -> StructType:\n",
    "        \"\"\"Return the schema for this data source.\"\"\"\n",
    "        # SOLUTION 1.3b: Return the schema\n",
    "        return get_dispatchregionsum_schema()\n",
    "\n",
    "    def reader(self, schema: StructType) -> DataSourceReader:\n",
    "        \"\"\"Create a reader for this data source.\"\"\"\n",
    "        # SOLUTION 1.3c: Create and return the reader\n",
    "        return NemwebReader(schema, self.options)\n",
    "\n",
    "\n",
    "# Test the complete implementation\n",
    "print(\"NemwebDataSource defined successfully!\")\n",
    "print(f\"  - name(): {NemwebDataSource.name()}\")\n",
    "ds = NemwebDataSource(options={})\n",
    "print(f\"  - schema(): {len(ds.schema().fields)} fields\")\n",
    "print(f\"  - reader(): {type(ds.reader(ds.schema())).__name__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_implementation():\n",
    "    \"\"\"Validate the custom data source implementation.\"\"\"\n",
    "    print(\"=\" * 60)\n",
    "    print(\"FINAL VALIDATION\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    checks = {\n",
    "        \"Part 1 - Schema (12 fields)\": False,\n",
    "        \"Part 2 - Partitions\": False,\n",
    "        \"Part 2 - Read (RecordBatch)\": False,\n",
    "        \"Part 3 - DataSource.name()\": False,\n",
    "        \"Part 3 - DataSource.schema()\": False,\n",
    "        \"Part 3 - DataSource.reader()\": False,\n",
    "    }\n",
    "\n",
    "    # Part 1: Schema\n",
    "    schema = get_dispatchregionsum_schema()\n",
    "    required = [\"TOTALDEMAND\", \"AVAILABLEGENERATION\", \"NETINTERCHANGE\"]\n",
    "    schema_ok = (\n",
    "        len(schema.fields) >= 12 and\n",
    "        all(f in [field.name for field in schema.fields] for f in required)\n",
    "    )\n",
    "    checks[\"Part 1 - Schema (12 fields)\"] = schema_ok\n",
    "\n",
    "    # Part 2: Partitions\n",
    "    reader = NemwebReader(schema, {\"regions\": \"NSW1,VIC1,QLD1\"})\n",
    "    partitions = reader.partitions()\n",
    "    checks[\"Part 2 - Partitions\"] = partitions is not None and len(partitions) == 3\n",
    "\n",
    "    # Part 2: Read (should yield RecordBatch)\n",
    "    if partitions:\n",
    "        try:\n",
    "            import pyarrow as pa\n",
    "            test_partition = NemwebPartition(\"NSW1\", \"2024-01-01\", \"2024-01-01\")\n",
    "            result = list(reader.read(test_partition))\n",
    "            # Check if we got a RecordBatch\n",
    "            checks[\"Part 2 - Read (RecordBatch)\"] = (\n",
    "                len(result) > 0 and \n",
    "                isinstance(result[0], pa.RecordBatch)\n",
    "            )\n",
    "        except Exception as e:\n",
    "            print(f\"Read check error: {e}\")\n",
    "\n",
    "    # Part 3: DataSource\n",
    "    try:\n",
    "        checks[\"Part 3 - DataSource.name()\"] = NemwebDataSource.name() == \"nemweb\"\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    try:\n",
    "        ds = NemwebDataSource(options={})\n",
    "        checks[\"Part 3 - DataSource.schema()\"] = len(ds.schema().fields) >= 12\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    try:\n",
    "        ds = NemwebDataSource(options={})\n",
    "        checks[\"Part 3 - DataSource.reader()\"] = ds.reader(schema) is not None\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    # Print results\n",
    "    print()\n",
    "    for check, passed in checks.items():\n",
    "        status = \"‚úÖ\" if passed else \"‚ùå\"\n",
    "        print(f\"  {status} {check}\")\n",
    "\n",
    "    all_passed = all(checks.values())\n",
    "    print()\n",
    "    if all_passed:\n",
    "        print(\"=\" * 60)\n",
    "        print(\"üéâ All checks passed!\")\n",
    "        print(\"=\" * 60)\n",
    "\n",
    "    return all_passed\n",
    "\n",
    "validate_implementation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Register and use the custom data source we built\n",
    "spark.dataSource.register(NemwebDataSource)\n",
    "\n",
    "# Calculate date range (use 2 days ago since CURRENT folder has ~7 days of data)\n",
    "from datetime import datetime, timedelta\n",
    "target_date = (datetime.now() - timedelta(days=2)).strftime(\"%Y-%m-%d\")\n",
    "\n",
    "# Read data using our custom datasource\n",
    "df = (spark.read\n",
    "      .format(\"nemweb\")\n",
    "      .option(\"regions\", \"NSW1\")\n",
    "      .option(\"start_date\", target_date)\n",
    "      .option(\"end_date\", target_date)\n",
    "      .load())\n",
    "\n",
    "# Display results\n",
    "print(f\"Row count: {df.count()}\")\n",
    "display(df.limit(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "You built a custom PySpark data source that fetches **live data** from NEMWEB!\n",
    "\n",
    "| Component | Purpose |\n",
    "|-----------|---------|\n",
    "| `DataSource.name()` | Format string for `spark.read.format(...)` |\n",
    "| `DataSource.schema()` | Define output columns and types |\n",
    "| `DataSource.reader()` | Create reader with options |\n",
    "| `DataSourceReader.partitions()` | Plan parallel work units |\n",
    "| `DataSourceReader.read()` | Fetch data and yield PyArrow RecordBatch |\n",
    "\n",
    "**Key Requirement:** Serverless/Spark Connect requires `read()` to yield **PyArrow RecordBatch** objects.\n",
    "The `parse_nemweb_to_arrow()` helper function handles all the type conversion for you.\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "Proceed to **Exercise 2** to integrate your data source with Lakeflow Pipelines."
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "dependencies": [
     "/Workspace/Users/david.okeeffe@databricks.com/.bundle/nemweb-lab/dev/files/databricks-nemweb-lab/artifacts/nemweb_datasource-2.10.11-py3-none-any.whl"
    ],
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "01_custom_source_solution",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

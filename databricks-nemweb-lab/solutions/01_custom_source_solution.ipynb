{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Solution: Exercise 1 - Building a Custom PySpark Data Source\n",
        "This notebook contains the complete solutions for Exercise 1.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Warm-up: Hello World Data Source\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "from pyspark.sql.datasource import DataSource, DataSourceReader\n",
        "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
        "\n",
        "from databricks.sdk.runtime import spark, display\n",
        "\n",
        "class HelloWorldDataSource(DataSource):\n",
        "    \"\"\"Minimal data source that generates greeting messages.\"\"\"\n",
        "\n",
        "    @classmethod\n",
        "    def name(cls) -> str:\n",
        "        return \"hello\"\n",
        "\n",
        "    def schema(self) -> StructType:\n",
        "        return StructType([\n",
        "            StructField(\"id\", IntegerType()),\n",
        "            StructField(\"message\", StringType()),\n",
        "        ])\n",
        "\n",
        "    def reader(self, schema: StructType) -> DataSourceReader:\n",
        "        return HelloWorldReader(self.options)\n",
        "\n",
        "\n",
        "class HelloWorldReader(DataSourceReader):\n",
        "    def __init__(self, options: dict):\n",
        "        self.count = int(options.get(\"count\", 5))\n",
        "\n",
        "    def read(self, partition):\n",
        "        for i in range(self.count):\n",
        "            yield (i, f\"Hello, World #{i}!\")\n",
        "\n",
        "spark.dataSource.register(HelloWorldDataSource)\n",
        "df = spark.read.format(\"hello\").option(\"count\", 3).load()\n",
        "display(df)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Solution 1.1: Complete Schema Definition\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "from pyspark.sql.datasource import DataSource, DataSourceReader, InputPartition\n",
        "from pyspark.sql.types import (\n",
        "    StructType, StructField, StringType, DoubleType, TimestampType\n",
        ")\n",
        "from typing import Iterator, Tuple\n",
        "from datetime import datetime, timedelta\n",
        "\n",
        "def get_dispatchregionsum_schema() -> StructType:\n",
        "    \"\"\"\n",
        "    Return the schema for DISPATCHREGIONSUM table.\n",
        "\n",
        "    Reference: MMS Electricity Data Model Report - DISPATCH package\n",
        "    \"\"\"\n",
        "    return StructType([\n",
        "        # Time and identification fields\n",
        "        StructField(\"SETTLEMENTDATE\", TimestampType(), True),\n",
        "        StructField(\"RUNNO\", StringType(), True),\n",
        "        StructField(\"REGIONID\", StringType(), True),\n",
        "        StructField(\"DISPATCHINTERVAL\", StringType(), True),\n",
        "        StructField(\"INTERVENTION\", StringType(), True),\n",
        "\n",
        "        # SOLUTION 1.1: Added measurement fields\n",
        "        StructField(\"TOTALDEMAND\", DoubleType(), True),\n",
        "        StructField(\"AVAILABLEGENERATION\", DoubleType(), True),\n",
        "        StructField(\"AVAILABLELOAD\", DoubleType(), True),\n",
        "        StructField(\"DEMANDFORECAST\", DoubleType(), True),\n",
        "        StructField(\"DISPATCHABLEGENERATION\", DoubleType(), True),\n",
        "        StructField(\"DISPATCHABLELOAD\", DoubleType(), True),\n",
        "        StructField(\"NETINTERCHANGE\", DoubleType(), True),\n",
        "    ])\n",
        "\n",
        "# Verify schema\n",
        "schema = get_dispatchregionsum_schema()\n",
        "print(f\"Schema has {len(schema.fields)} fields (expected: 12)\")\n",
        "for field in schema.fields:\n",
        "    print(f\"  - {field.name}: {field.dataType}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Option: Inline Datasource Implementation (For Debugging)\n",
        "\n",
        "The following cell contains the complete datasource implementation inline.\n",
        "This allows Databricks Assistant to help debug timestamp conversion issues.\n",
        "\n",
        "**To use:** Uncomment the code in the next cell and comment out the package import in the cell after it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Complete inline datasource implementation\n",
        "# Uncomment this block and comment out the package import below to use inline code\n",
        "# This gives Databricks Assistant full context to help debug timestamp issues\n",
        "\n",
        "# import csv\n",
        "# import io\n",
        "# import re\n",
        "# import time\n",
        "# import zipfile\n",
        "# from datetime import datetime, timedelta\n",
        "# from typing import Iterator, Tuple, Optional\n",
        "# from urllib.request import urlopen, Request\n",
        "# from urllib.error import HTTPError, URLError\n",
        "# from pyspark.sql.types import StructType, TimestampType, StringType, DoubleType, IntegerType\n",
        "# \n",
        "# # Constants\n",
        "# NEMWEB_CURRENT_URL = \"https://www.nemweb.com.au/REPORTS/CURRENT\"\n",
        "# REQUEST_TIMEOUT = 30\n",
        "# USER_AGENT = \"DatabricksNemwebLab/1.0\"\n",
        "# MAX_RETRIES = 3\n",
        "# DEBUG_LOG_PATH = \"/tmp/nemweb_debug.log\"\n",
        "# \n",
        "# TABLE_CONFIG = {\n",
        "#     \"DISPATCHREGIONSUM\": {\n",
        "#         \"folder\": \"DispatchIS_Reports\",\n",
        "#         \"file_prefix\": \"DISPATCHIS\",\n",
        "#         \"record_type\": \"DISPATCH,REGIONSUM\"\n",
        "#     },\n",
        "# }\n",
        "# \n",
        "# def _debug_log(msg: str) -> None:\n",
        "#     try:\n",
        "#         with open(DEBUG_LOG_PATH, \"a\") as f:\n",
        "#             f.write(f\"{msg}\\n\")\n",
        "#     except Exception:\n",
        "#         pass\n",
        "# \n",
        "# def _get_sample_data(table: str, region: Optional[str] = None) -> list[dict]:\n",
        "#     sample = [\n",
        "#         {\"SETTLEMENTDATE\": \"2024-01-01 00:05:00\", \"RUNNO\": \"1\", \"REGIONID\": \"NSW1\",\n",
        "#          \"DISPATCHINTERVAL\": \"1\", \"INTERVENTION\": \"0\", \"TOTALDEMAND\": \"7500.5\",\n",
        "#          \"AVAILABLEGENERATION\": \"8000.0\", \"NETINTERCHANGE\": \"-200.5\"},\n",
        "#         {\"SETTLEMENTDATE\": \"2024-01-01 00:05:00\", \"RUNNO\": \"1\", \"REGIONID\": \"VIC1\",\n",
        "#          \"DISPATCHINTERVAL\": \"1\", \"INTERVENTION\": \"0\", \"TOTALDEMAND\": \"5200.3\",\n",
        "#          \"AVAILABLEGENERATION\": \"5500.0\", \"NETINTERCHANGE\": \"150.2\"},\n",
        "#     ]\n",
        "#     if region:\n",
        "#         sample = [row for row in sample if row[\"REGIONID\"] == region]\n",
        "#     return sample\n",
        "# \n",
        "# def fetch_with_retry(url: str, max_retries: int = MAX_RETRIES) -> bytes:\n",
        "#     last_error = None\n",
        "#     for attempt in range(max_retries):\n",
        "#         try:\n",
        "#             request = Request(url, headers={\"User-Agent\": USER_AGENT})\n",
        "#             with urlopen(request, timeout=REQUEST_TIMEOUT) as response:\n",
        "#                 return response.read()\n",
        "#         except (HTTPError, URLError) as e:\n",
        "#             last_error = e\n",
        "#             if isinstance(e, HTTPError) and e.code == 404:\n",
        "#                 raise\n",
        "#             if attempt < max_retries - 1:\n",
        "#                 time.sleep(1.0 * (2 ** attempt))\n",
        "#     raise last_error\n",
        "# \n",
        "# def _parse_nemweb_csv_file(csv_file, record_type: str = None) -> list[dict]:\n",
        "#     text = csv_file.read().decode(\"utf-8\")\n",
        "#     if not record_type:\n",
        "#         return list(csv.DictReader(io.StringIO(text)))\n",
        "#     rows, headers = [], None\n",
        "#     for parts in csv.reader(io.StringIO(text)):\n",
        "#         if not parts:\n",
        "#             continue\n",
        "#         row_type = parts[0].strip().upper()\n",
        "#         if row_type == \"I\" and len(parts) > 2:\n",
        "#             if f\"{parts[1]},{parts[2]}\" == record_type:\n",
        "#                 headers = parts[4:]\n",
        "#         elif row_type == \"D\" and headers and len(parts) > 2:\n",
        "#             if f\"{parts[1]},{parts[2]}\" == record_type:\n",
        "#                 values = parts[4:]\n",
        "#                 row_dict = dict(zip(headers, values))\n",
        "#                 for header in headers[len(values):]:\n",
        "#                     row_dict[header] = None\n",
        "#                 rows.append(row_dict)\n",
        "#     return rows\n",
        "# \n",
        "# def _fetch_and_extract_zip(url: str, record_type: str = None) -> list[dict]:\n",
        "#     raw_data = fetch_with_retry(url)\n",
        "#     zip_data = io.BytesIO(raw_data)\n",
        "#     rows = []\n",
        "#     with zipfile.ZipFile(zip_data) as zf:\n",
        "#         for name in zf.namelist():\n",
        "#             if name.endswith((\".zip\", \".ZIP\")):\n",
        "#                 with zf.open(name) as nested_zip_file:\n",
        "#                     nested_zip_data = io.BytesIO(nested_zip_file.read())\n",
        "#                     with zipfile.ZipFile(nested_zip_data) as nested_zf:\n",
        "#                         for nested_name in nested_zf.namelist():\n",
        "#                             if nested_name.endswith((\".CSV\", \".csv\")):\n",
        "#                                 with nested_zf.open(nested_name) as csv_file:\n",
        "#                                     rows.extend(_parse_nemweb_csv_file(csv_file, record_type))\n",
        "#             elif name.endswith((\".CSV\", \".csv\")):\n",
        "#                 with zf.open(name) as csv_file:\n",
        "#                     rows.extend(_parse_nemweb_csv_file(csv_file, record_type))\n",
        "#     return rows\n",
        "# \n",
        "# def fetch_nemweb_current(table: str, region: Optional[str] = None, max_files: int = 6,\n",
        "#                          use_sample: bool = False, debug: bool = False) -> list[dict]:\n",
        "#     if use_sample:\n",
        "#         return _get_sample_data(table, region)\n",
        "#     if table not in TABLE_CONFIG:\n",
        "#         raise ValueError(f\"Unsupported table: {table}\")\n",
        "#     config = TABLE_CONFIG[table]\n",
        "#     current_url = f\"{NEMWEB_CURRENT_URL}/{config['folder']}/\"\n",
        "#     try:\n",
        "#         request = Request(current_url, headers={\"User-Agent\": USER_AGENT})\n",
        "#         with urlopen(request, timeout=REQUEST_TIMEOUT) as response:\n",
        "#             html = response.read().decode('utf-8')\n",
        "#     except (HTTPError, URLError) as e:\n",
        "#         if debug:\n",
        "#             print(f\"[NEMWEB] ERROR: {e}\")\n",
        "#         raise\n",
        "#     pattern = rf'(PUBLIC_{config[\"file_prefix\"]}_\\d{{12}}_\\d+\\.zip)'\n",
        "#     matches = sorted(set(re.findall(pattern, html, re.IGNORECASE)), reverse=True)[:max_files]\n",
        "#     rows = []\n",
        "#     for filename in matches:\n",
        "#         url = f\"{NEMWEB_CURRENT_URL}/{config['folder']}/{filename}\"\n",
        "#         try:\n",
        "#             data = _fetch_and_extract_zip(url, config.get(\"record_type\"))\n",
        "#             if region:\n",
        "#                 data = [row for row in data if row.get(\"REGIONID\") == region]\n",
        "#             rows.extend(data)\n",
        "#         except Exception as e:\n",
        "#             if debug:\n",
        "#                 print(f\"[NEMWEB] ERROR fetching {filename}: {e}\")\n",
        "#     return rows\n",
        "# \n",
        "# # Timestamp parsing and conversion functions\n",
        "# def _parse_timestamp_value(ts_str: str) -> Optional[datetime]:\n",
        "#     if not ts_str:\n",
        "#         return None\n",
        "#     ts_str = str(ts_str).strip().strip('\"').strip(\"'\").strip()\n",
        "#     if not ts_str:\n",
        "#         return None\n",
        "#     for fmt in [\"%Y/%m/%d %H:%M:%S\", \"%Y-%m-%d %H:%M:%S\", \"%Y/%m/%d %H:%M\", \"%Y-%m-%d %H:%M\"]:\n",
        "#         try:\n",
        "#             return datetime.strptime(ts_str, fmt)\n",
        "#         except ValueError:\n",
        "#             continue\n",
        "#     return None\n",
        "# \n",
        "# def _to_python_scalar(v: any, spark_type: any = None) -> any:\n",
        "#     if v is None:\n",
        "#         return None\n",
        "#     if isinstance(v, datetime):\n",
        "#         if v.tzinfo is not None:\n",
        "#             import datetime as dt\n",
        "#             return v.astimezone(dt.timezone.utc).replace(tzinfo=None)\n",
        "#         return v\n",
        "#     try:\n",
        "#         import pandas as pd\n",
        "#         if isinstance(v, pd.Timestamp):\n",
        "#             return None if pd.isna(v) else v.to_pydatetime()\n",
        "#     except Exception:\n",
        "#         pass\n",
        "#     try:\n",
        "#         import numpy as np\n",
        "#         if isinstance(v, (np.datetime64,)):\n",
        "#             if np.isnat(v):\n",
        "#                 return None\n",
        "#             try:\n",
        "#                 import pandas as pd\n",
        "#                 ts = pd.to_datetime(v, utc=False)\n",
        "#                 return None if pd.isna(ts) else ts.to_pydatetime()\n",
        "#             except Exception:\n",
        "#                 try:\n",
        "#                     ts_str = np.datetime_as_string(v, unit='us', timezone='naive')\n",
        "#                     return datetime.fromisoformat(ts_str.replace('Z', ''))\n",
        "#                 except Exception:\n",
        "#                     return None\n",
        "#         if isinstance(v, (np.bool_, np.integer, np.floating)):\n",
        "#             return bool(v) if isinstance(v, np.bool_) else (int(v) if isinstance(v, np.integer) else float(v))\n",
        "#     except Exception:\n",
        "#         pass\n",
        "#     if spark_type and isinstance(spark_type, TimestampType):\n",
        "#         if isinstance(v, datetime):\n",
        "#             return v\n",
        "#         if v is None:\n",
        "#             return None\n",
        "#         if isinstance(v, str):\n",
        "#             parsed = _parse_timestamp_value(v)\n",
        "#             return parsed if parsed is not None else None\n",
        "#         return None\n",
        "#     return v if isinstance(v, str) else v\n",
        "# \n",
        "# def _convert_value(value, spark_type):\n",
        "#     if value is None or value == \"\":\n",
        "#         return None\n",
        "#     try:\n",
        "#         str_value = str(value).strip()\n",
        "#     except Exception:\n",
        "#         return None\n",
        "#     if str_value == \"\":\n",
        "#         return None\n",
        "#     if isinstance(spark_type, TimestampType):\n",
        "#         parsed_ts = _parse_timestamp_value(str_value)\n",
        "#         result = _to_python_scalar(parsed_ts, spark_type)\n",
        "#         if result is not None and not isinstance(result, datetime):\n",
        "#             if isinstance(result, str):\n",
        "#                 result = _parse_timestamp_value(result)\n",
        "#             else:\n",
        "#                 return None\n",
        "#         return result\n",
        "#     elif isinstance(spark_type, StringType):\n",
        "#         return str_value.replace(\"/\", \"-\") if \"/\" in str_value else str_value\n",
        "#     elif isinstance(spark_type, DoubleType):\n",
        "#         try:\n",
        "#             return _to_python_scalar(float(str_value), spark_type)\n",
        "#         except (ValueError, TypeError):\n",
        "#             return None\n",
        "#     elif isinstance(spark_type, IntegerType):\n",
        "#         try:\n",
        "#             return _to_python_scalar(int(float(str_value)), spark_type)\n",
        "#         except (ValueError, TypeError):\n",
        "#             return None\n",
        "#     return str_value\n",
        "# \n",
        "# def _validate_tuple_types(tuple_values: list, schema) -> Optional[tuple]:\n",
        "#     validated = []\n",
        "#     for field, value in zip(schema.fields, tuple_values):\n",
        "#         if isinstance(field.dataType, TimestampType):\n",
        "#             if value is None:\n",
        "#                 validated.append(None)\n",
        "#             elif isinstance(value, datetime):\n",
        "#                 if value.tzinfo is not None:\n",
        "#                     import datetime as dt\n",
        "#                     validated.append(value.astimezone(dt.timezone.utc).replace(tzinfo=None))\n",
        "#                 else:\n",
        "#                     validated.append(value)\n",
        "#             else:\n",
        "#                 converted_value = None\n",
        "#                 try:\n",
        "#                     converted_value = _parse_timestamp_value(value) if isinstance(value, str) else _to_python_scalar(value, TimestampType())\n",
        "#                     if converted_value is not None and isinstance(converted_value, datetime):\n",
        "#                         if converted_value.tzinfo is not None:\n",
        "#                             import datetime as dt\n",
        "#                             converted_value = converted_value.astimezone(dt.timezone.utc).replace(tzinfo=None)\n",
        "#                         validated.append(converted_value)\n",
        "#                     else:\n",
        "#                         validated.append(None)\n",
        "#                 except Exception:\n",
        "#                     validated.append(None)\n",
        "#         else:\n",
        "#             validated.append(value)\n",
        "#     result = tuple(validated)\n",
        "#     for field, val in zip(schema.fields, result):\n",
        "#         if isinstance(field.dataType, TimestampType):\n",
        "#             if val is not None and not isinstance(val, datetime):\n",
        "#                 return None\n",
        "#     return result\n",
        "# \n",
        "# def parse_nemweb_csv(data: list[dict], schema) -> Iterator[Tuple]:\n",
        "#     _debug_log(f\"=== parse_nemweb_csv started ===\")\n",
        "#     _debug_log(f\"Processing {len(data)} rows\")\n",
        "#     field_names = [field.name for field in schema.fields]\n",
        "#     field_types = {field.name: field.dataType for field in schema.fields}\n",
        "#     row_num = 0\n",
        "#     for row in data:\n",
        "#         row_num += 1\n",
        "#         try:\n",
        "#             values = []\n",
        "#             for name in field_names:\n",
        "#                 raw_value = row.get(name)\n",
        "#                 if raw_value is None or raw_value == \"\":\n",
        "#                     values.append(None)\n",
        "#                 else:\n",
        "#                     converted = _convert_value(raw_value, field_types[name])\n",
        "#                     coerced = _to_python_scalar(converted, field_types[name])\n",
        "#                     if isinstance(field_types[name], TimestampType):\n",
        "#                         if coerced is None:\n",
        "#                             values.append(None)\n",
        "#                         elif isinstance(coerced, datetime):\n",
        "#                             if coerced.tzinfo is not None:\n",
        "#                                 import datetime as dt\n",
        "#                                 values.append(coerced.astimezone(dt.timezone.utc).replace(tzinfo=None))\n",
        "#                             else:\n",
        "#                                 values.append(coerced)\n",
        "#                         else:\n",
        "#                             try:\n",
        "#                                 final_val = _parse_timestamp_value(coerced) if isinstance(coerced, str) else _to_python_scalar(coerced, TimestampType())\n",
        "#                                 if final_val is None:\n",
        "#                                     values.append(None)\n",
        "#                                 elif isinstance(final_val, datetime):\n",
        "#                                     if final_val.tzinfo is not None:\n",
        "#                                         import datetime as dt\n",
        "#                                         values.append(final_val.astimezone(dt.timezone.utc).replace(tzinfo=None))\n",
        "#                                     else:\n",
        "#                                         values.append(final_val)\n",
        "#                                 else:\n",
        "#                                     values.append(None)\n",
        "#                             except Exception:\n",
        "#                                 values.append(None)\n",
        "#                     else:\n",
        "#                         values.append(coerced)\n",
        "#             validated_tuple = _validate_tuple_types(values, schema)\n",
        "#             if validated_tuple is None:\n",
        "#                 continue\n",
        "#             final_values = []\n",
        "#             for idx, (field, val) in enumerate(zip(schema.fields, validated_tuple)):\n",
        "#                 if isinstance(field.dataType, TimestampType):\n",
        "#                     if val is None:\n",
        "#                         final_values.append(None)\n",
        "#                     elif isinstance(val, datetime):\n",
        "#                         if val.tzinfo is not None:\n",
        "#                             import datetime as dt\n",
        "#                             final_values.append(val.astimezone(dt.timezone.utc).replace(tzinfo=None))\n",
        "#                         else:\n",
        "#                             final_values.append(val)\n",
        "#                     else:\n",
        "#                         final_values.append(None)\n",
        "#                 else:\n",
        "#                     final_values.append(val)\n",
        "#             yield tuple(final_values)\n",
        "#         except Exception as e:\n",
        "#             _debug_log(f\"ROW {row_num} ERROR: {e}\")\n",
        "#             continue\n",
        "#     _debug_log(f\"=== parse_nemweb_csv complete: {row_num} rows ===\")\n",
        "# \n",
        "# def get_version() -> str:\n",
        "#     return \"2.10.8-inline\"\n",
        "# \n",
        "# print(\"‚úÖ Inline datasource functions loaded\")\n",
        "# print(f\"üìù Debug log: {DEBUG_LOG_PATH}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Solution 1.2: Partition Planning and Data Reading\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# Import helper functions\n",
        "import sys\n",
        "import os\n",
        "\n",
        "notebook_path = dbutils.notebook.entry_point.getDbutils().notebook().getContext().notebookPath().get()\n",
        "repo_root = str(os.path.dirname(os.path.dirname(notebook_path)))\n",
        "sys.path.insert(0, f\"/Workspace{repo_root}/src\")\n",
        "\n",
        "from nemweb_utils import fetch_nemweb_current, parse_nemweb_csv, get_version\n",
        "\n",
        "# Output version for debugging\n",
        "print(f\"nemweb_utils version: {get_version()}\")\n",
        "\n",
        "# Quick test\n",
        "test_data = fetch_nemweb_current(\n",
        "    table=\"DISPATCHREGIONSUM\",\n",
        "    region=\"NSW1\",\n",
        "    max_files=2,\n",
        "    use_sample=True\n",
        ")\n",
        "print(f\"Helper function works! Got {len(test_data)} rows\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "class NemwebPartition(InputPartition):\n",
        "    \"\"\"\n",
        "    Represents one partition of NEMWEB data.\n",
        "    Each partition handles one region's data.\n",
        "    \"\"\"\n",
        "    def __init__(self, region: str, start_date: str, end_date: str):\n",
        "        self.region = region\n",
        "        self.start_date = start_date\n",
        "        self.end_date = end_date\n",
        "\n",
        "\n",
        "class NemwebReader(DataSourceReader):\n",
        "    \"\"\"\n",
        "    Reader for NEMWEB data source.\n",
        "\n",
        "    The reader has two jobs:\n",
        "    1. partitions() - Plan the work (called on driver)\n",
        "    2. read() - Do the work (called on workers)\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, schema: StructType, options: dict):\n",
        "        self.schema = schema\n",
        "        self.options = options\n",
        "        self.regions = options.get(\"regions\", \"NSW1,QLD1,SA1,VIC1,TAS1\").split(\",\")\n",
        "        yesterday = (datetime.now() - timedelta(days=1)).strftime(\"%Y-%m-%d\")\n",
        "        self.start_date = options.get(\"start_date\", yesterday)\n",
        "        self.end_date = options.get(\"end_date\", yesterday)\n",
        "\n",
        "    def partitions(self) -> list[InputPartition]:\n",
        "        \"\"\"\n",
        "        Plan partitions for parallel reading.\n",
        "\n",
        "        SOLUTION 1.2a: Create one partition per region\n",
        "        \"\"\"\n",
        "        partitions = []\n",
        "\n",
        "        for region in self.regions:\n",
        "            partition = NemwebPartition(\n",
        "                region=region.strip(),\n",
        "                start_date=self.start_date,\n",
        "                end_date=self.end_date\n",
        "            )\n",
        "            partitions.append(partition)\n",
        "\n",
        "        return partitions\n",
        "\n",
        "    def read(self, partition: NemwebPartition) -> Iterator[Tuple]:\n",
        "        \"\"\"\n",
        "        Read data for a single partition (runs on workers).\n",
        "\n",
        "        SOLUTION 1.2b: Fetch and parse NEMWEB data\n",
        "        \"\"\"\n",
        "        # Fetch live data from NEMWEB CURRENT folder\n",
        "        data = fetch_nemweb_current(\n",
        "            table=\"DISPATCHREGIONSUM\",\n",
        "            region=partition.region,\n",
        "            max_files=6\n",
        "        )\n",
        "\n",
        "        # Convert to tuples matching schema\n",
        "        for row_tuple in parse_nemweb_csv(data, self.schema):\n",
        "            yield row_tuple\n",
        "\n",
        "\n",
        "# Test partition planning\n",
        "test_options = {\"regions\": \"NSW1,VIC1,QLD1\"}\n",
        "reader = NemwebReader(schema, test_options)\n",
        "partitions = reader.partitions()\n",
        "\n",
        "print(f\"Created {len(partitions)} partitions (expected: 3)\")\n",
        "for p in partitions:\n",
        "    print(f\"  - Region: {p.region}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Solution 1.3: Complete DataSource Class\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "class NemwebDataSource(DataSource):\n",
        "    \"\"\"\n",
        "    Custom PySpark Data Source for AEMO NEMWEB electricity market data.\n",
        "\n",
        "    Usage:\n",
        "        spark.dataSource.register(NemwebDataSource)\n",
        "        df = spark.read.format(\"nemweb\").option(\"regions\", \"NSW1,VIC1\").load()\n",
        "\n",
        "    Options:\n",
        "        - regions: Comma-separated list of NEM regions (default: all 5)\n",
        "        - start_date: Start date in YYYY-MM-DD format\n",
        "        - end_date: End date in YYYY-MM-DD format\n",
        "    \"\"\"\n",
        "\n",
        "    @classmethod\n",
        "    def name(cls) -> str:\n",
        "        \"\"\"Return the format name used in spark.read.format(\"...\").\"\"\"\n",
        "        # SOLUTION 1.3a: Return format name\n",
        "        return \"nemweb\"\n",
        "\n",
        "    def schema(self) -> StructType:\n",
        "        \"\"\"Return the schema for this data source.\"\"\"\n",
        "        # SOLUTION 1.3b: Return schema\n",
        "        return get_dispatchregionsum_schema()\n",
        "\n",
        "    def reader(self, schema: StructType) -> DataSourceReader:\n",
        "        \"\"\"Create a reader for this data source.\"\"\"\n",
        "        # SOLUTION 1.3c: Create reader with schema and options\n",
        "        return NemwebReader(schema, self.options)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Register and Test\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# Register the data source with Spark\n",
        "spark.dataSource.register(NemwebDataSource)\n",
        "\n",
        "# Read data using your custom data source!\n",
        "df = (spark.read\n",
        "      .format(\"nemweb\")\n",
        "      .option(\"regions\", \"NSW1\")\n",
        "      .load())\n",
        "\n",
        "print(f\"Row count: {df.count()}\")\n",
        "display(df.limit(5))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Validation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "def validate_implementation():\n",
        "    \"\"\"Validate the custom data source implementation.\"\"\"\n",
        "    print(\"=\" * 60)\n",
        "    print(\"FINAL VALIDATION\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    checks = {\n",
        "        \"Part 1 - Schema (12 fields)\": False,\n",
        "        \"Part 2 - Partitions\": False,\n",
        "        \"Part 2 - Read\": False,\n",
        "        \"Part 3 - DataSource.name()\": False,\n",
        "        \"Part 3 - DataSource.schema()\": False,\n",
        "        \"Part 3 - DataSource.reader()\": False,\n",
        "    }\n",
        "\n",
        "    # Part 1: Schema\n",
        "    schema = get_dispatchregionsum_schema()\n",
        "    required = [\"TOTALDEMAND\", \"AVAILABLEGENERATION\", \"NETINTERCHANGE\"]\n",
        "    schema_ok = (\n",
        "        len(schema.fields) >= 12 and\n",
        "        all(f in [field.name for field in schema.fields] for f in required)\n",
        "    )\n",
        "    checks[\"Part 1 - Schema (12 fields)\"] = schema_ok\n",
        "\n",
        "    # Part 2: Partitions\n",
        "    reader = NemwebReader(schema, {\"regions\": \"NSW1,VIC1,QLD1\"})\n",
        "    partitions = reader.partitions()\n",
        "    checks[\"Part 2 - Partitions\"] = partitions is not None and len(partitions) == 3\n",
        "\n",
        "    # Part 2: Read\n",
        "    if partitions:\n",
        "        try:\n",
        "            test_partition = NemwebPartition(\"NSW1\", \"2024-01-01\", \"2024-01-01\")\n",
        "            result = list(reader.read(test_partition))\n",
        "            checks[\"Part 2 - Read\"] = len(result) > 0\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "    # Part 3: DataSource\n",
        "    try:\n",
        "        checks[\"Part 3 - DataSource.name()\"] = NemwebDataSource.name() == \"nemweb\"\n",
        "    except:\n",
        "        pass\n",
        "\n",
        "    try:\n",
        "        ds = NemwebDataSource(options={})\n",
        "        checks[\"Part 3 - DataSource.schema()\"] = len(ds.schema().fields) >= 12\n",
        "    except:\n",
        "        pass\n",
        "\n",
        "    try:\n",
        "        ds = NemwebDataSource(options={})\n",
        "        checks[\"Part 3 - DataSource.reader()\"] = ds.reader(schema) is not None\n",
        "    except:\n",
        "        pass\n",
        "\n",
        "    # Print results\n",
        "    print()\n",
        "    for check, passed in checks.items():\n",
        "        status = \"‚úÖ\" if passed else \"‚ùå\"\n",
        "        print(f\"  {status} {check}\")\n",
        "\n",
        "    all_passed = all(checks.values())\n",
        "    print()\n",
        "    if all_passed:\n",
        "        print(\"=\" * 60)\n",
        "        print(\"üéâ All checks passed!\")\n",
        "        print(\"=\" * 60)\n",
        "\n",
        "    return all_passed\n",
        "\n",
        "validate_implementation()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary\n",
        "| Component | Purpose |\n",
        "|-----------|---------|\n",
        "| `DataSource.name()` | Format string for `spark.read.format(...)` |\n",
        "| `DataSource.schema()` | Define output columns and types |\n",
        "| `DataSource.reader()` | Create reader with options |\n",
        "| `DataSourceReader.partitions()` | Plan parallel work units |\n",
        "| `DataSourceReader.read()` | Fetch and yield data (runs on workers) |\n",
        "## Compare to Production\n",
        "Your implementation is a simplified version. The production code in\n",
        "`src/nemweb_datasource_arrow.py` adds:\n",
        "- **PyArrow RecordBatch** for zero-copy transfer (Serverless compatible)\n",
        "- **Volume mode** with parallel downloads to UC Volume\n",
        "- **Multiple tables** (DISPATCHREGIONSUM, DISPATCHPRICE, TRADINGPRICE)\n",
        "- **Retry logic** with exponential backoff\n"
      ]
    }
  ],
  "metadata": {
    "application/vnd.databricks.v1+notebook": {
      "computePreferences": {
        "hardware": {
          "accelerator": null,
          "gpuPoolId": null,
          "memory": null
        }
      },
      "dashboards": [],
      "environmentMetadata": {
        "base_environment": "",
        "dependencies": [
          "/Workspace/Users/david.okeeffe@databricks.com/.bundle/nemweb-lab/dev/files/databricks-nemweb-lab/artifacts/nemweb_datasource-2.10.7-py3-none-any.whl"
        ],
        "environment_version": "4"
      },
      "inputWidgetPreferences": null,
      "language": "python",
      "notebookMetadata": {
        "pythonIndentUnit": 4
      },
      "notebookName": "01_custom_source_solution",
      "widgets": {}
    },
    "language_info": {
      "name": "python"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 4
}

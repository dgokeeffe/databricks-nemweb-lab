{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solution: Exercise 1 - Building a Custom PySpark Data Source\n",
    "\n",
    "**Time:** 15 minutes\n",
    "\n",
    "This notebook contains the complete solutions for Exercise 1.\n",
    "\n",
    "## Learning Objectives\n",
    "1. Understand the Python Data Source API architecture\n",
    "2. Implement schema definition for external data\n",
    "3. Create partition-aware data reading for parallelism\n",
    "4. Register and use your custom data source\n",
    "\n",
    "## Reference\n",
    "- [Python Data Source API Docs](https://docs.databricks.com/en/pyspark/datasources.html)\n",
    "- [Example Implementations (GitHub)](https://github.com/allisonwang-db/pyspark-data-sources)\n",
    "- Production implementation: `src/nemweb_datasource_arrow.py`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Warm-up: Hello World Data Source (2 minutes)\n",
    "\n",
    "Let's start with the **simplest possible** custom data source to understand the API.\n",
    "\n",
    "A custom data source needs just **3 components**:\n",
    "1. `name()` - The format string for `spark.read.format(\"name\")`\n",
    "2. `schema()` - What columns and types your data has\n",
    "3. `reader()` - Creates a reader that fetches the data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Insight\n",
    "\n",
    "That's it! A custom data source is:\n",
    "- **DataSource class**: Declares the name, schema, and creates a reader\n",
    "- **DataSourceReader class**: Has a `read()` method that yields tuples\n",
    "\n",
    "The tuples you yield **must match** the schema field order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pyspark.sql.datasource import DataSource, DataSourceReader\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
    "\n",
    "from databricks.sdk.runtime import spark, display\n",
    "\n",
    "class HelloWorldDataSource(DataSource):\n",
    "    \"\"\"Minimal data source that generates greeting messages.\"\"\"\n",
    "\n",
    "    @classmethod\n",
    "    def name(cls) -> str:\n",
    "        return \"hello\"\n",
    "\n",
    "    def schema(self) -> StructType:\n",
    "        return StructType([\n",
    "            StructField(\"id\", IntegerType()),\n",
    "            StructField(\"message\", StringType()),\n",
    "        ])\n",
    "\n",
    "    def reader(self, schema: StructType) -> DataSourceReader:\n",
    "        return HelloWorldReader(self.options)\n",
    "\n",
    "\n",
    "class HelloWorldReader(DataSourceReader):\n",
    "    def __init__(self, options: dict):\n",
    "        self.count = int(options.get(\"count\", 5))\n",
    "\n",
    "    def read(self, partition):\n",
    "        for i in range(self.count):\n",
    "            yield (i, f\"Hello, World #{i}!\")\n",
    "\n",
    "spark.dataSource.register(HelloWorldDataSource)\n",
    "df = spark.read.format(\"hello\").option(\"count\", 3).load()\n",
    "display(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Now Let's Build a Real One: NEMWEB Data Source\n",
    "\n",
    "AEMO NEMWEB publishes Australia's National Electricity Market data.\n",
    "We'll create a data source that fetches **live data** from their HTTP API.\n",
    "\n",
    "Key differences from Hello World:\n",
    "- **Real schema** based on AEMO's MMS data model\n",
    "- **Partitions** for parallel reading (one per NEM region)\n",
    "- **HTTP fetching** from https://www.nemweb.com.au/\n",
    "\n",
    "> **Reference:** See how the production code does it in `src/nemweb_datasource_arrow.py`\n",
    "\n",
    "---\n",
    "## Part 1: Define the Schema (3 minutes)\n",
    "\n",
    "The DISPATCHREGIONSUM table contains regional dispatch summary data.\n",
    "Let's define its schema using Spark types.\n",
    "\n",
    "> **Reference:** [MMS Data Model - DISPATCH package](https://nemweb.com.au/Reports/Current/MMSDataModelReport/)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pyspark.sql.datasource import DataSource, DataSourceReader, InputPartition\n",
    "from pyspark.sql.types import (\n",
    "    StructType, StructField, StringType, DoubleType, TimestampType\n",
    ")\n",
    "from typing import Iterator, Tuple\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "def get_dispatchregionsum_schema() -> StructType:\n",
    "    \"\"\"\n",
    "    Return the schema for DISPATCHREGIONSUM table.\n",
    "\n",
    "    Reference: MMS Electricity Data Model Report - DISPATCH package\n",
    "    \"\"\"\n",
    "    return StructType([\n",
    "        # Time and identification fields\n",
    "        StructField(\"SETTLEMENTDATE\", TimestampType(), True),\n",
    "        StructField(\"RUNNO\", StringType(), True),\n",
    "        StructField(\"REGIONID\", StringType(), True),\n",
    "        StructField(\"DISPATCHINTERVAL\", StringType(), True),\n",
    "        StructField(\"INTERVENTION\", StringType(), True),\n",
    "\n",
    "        # SOLUTION 1.1: Added measurement fields\n",
    "        StructField(\"TOTALDEMAND\", DoubleType(), True),\n",
    "        StructField(\"AVAILABLEGENERATION\", DoubleType(), True),\n",
    "        StructField(\"AVAILABLELOAD\", DoubleType(), True),\n",
    "        StructField(\"DEMANDFORECAST\", DoubleType(), True),\n",
    "        StructField(\"DISPATCHABLEGENERATION\", DoubleType(), True),\n",
    "        StructField(\"DISPATCHABLELOAD\", DoubleType(), True),\n",
    "        StructField(\"NETINTERCHANGE\", DoubleType(), True),\n",
    "    ])\n",
    "\n",
    "# Verify schema\n",
    "schema = get_dispatchregionsum_schema()\n",
    "print(f\"Schema has {len(schema.fields)} fields (expected: 12)\")\n",
    "for field in schema.fields:\n",
    "    print(f\"  - {field.name}: {field.dataType}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Add Partitioning and Data Reading (5 minutes)\n",
    "\n",
    "Spark achieves parallelism by dividing work into **partitions**.\n",
    "Each partition can be processed independently on different cores/nodes.\n",
    "\n",
    "For NEMWEB, we'll create one partition per NEM region:\n",
    "- NSW1 (New South Wales)\n",
    "- QLD1 (Queensland)\n",
    "- SA1 (South Australia)\n",
    "- VIC1 (Victoria)\n",
    "- TAS1 (Tasmania)\n",
    "\n",
    "You need to implement two methods:\n",
    "1. **`partitions()`** - Plan the work (runs on driver)\n",
    "2. **`read()`** - Do the work (runs on workers)\n",
    "\n",
    "### Helper Functions (Provided)\n",
    "\n",
    "NEMWEB data comes as CSV files inside ZIP archives with a complex multi-record format.\n",
    "We've provided helper functions so you can focus on the **Data Source API**.\n",
    "\n",
    "**Note:** The helper functions fetch **REAL data** from AEMO NEMWEB CURRENT folder\n",
    "(last ~7 days of 5-minute interval files). This ensures you're working with actual\n",
    "electricity market data.\n",
    "\n",
    "**Important:** Real AEMO DISPATCHREGIONSUM data contains ~130+ columns, but our schema\n",
    "only defines 12 key fields. The `parse_nemweb_csv()` function automatically filters\n",
    "to only the fields in your schema - extra columns are ignored, and missing columns\n",
    "are set to `None` (which is fine since all fields are nullable).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Import helper functions\n",
    "import sys\n",
    "import os\n",
    "\n",
    "notebook_path = dbutils.notebook.entry_point.getDbutils().notebook().getContext().notebookPath().get()\n",
    "repo_root = str(os.path.dirname(os.path.dirname(notebook_path)))\n",
    "sys.path.insert(0, f\"/Workspace{repo_root}/src\")\n",
    "\n",
    "from nemweb_utils import fetch_nemweb_current, parse_nemweb_csv, get_version\n",
    "\n",
    "# Output version for debugging\n",
    "print(f\"nemweb_utils version: {get_version()}\")\n",
    "\n",
    "# Quick test - these helpers handle HTTP fetching and CSV parsing\n",
    "# This fetches REAL data from AEMO NEMWEB CURRENT folder (last ~7 days)\n",
    "test_data = fetch_nemweb_current(\n",
    "    table=\"DISPATCHREGIONSUM\",\n",
    "    region=\"NSW1\",\n",
    "    max_files=2,\n",
    "    use_sample=False,  # Fetch real current files from AEMO\n",
    "    debug=True  # Print debug info\n",
    ")\n",
    "print(f\"Helper function works! Got {len(test_data)} rows\")\n",
    "if test_data:\n",
    "    # Note: Real AEMO data has MANY more columns (~130+) than our 12-field schema\n",
    "    # parse_nemweb_csv() will filter to only the fields in our schema\n",
    "    all_keys = list(test_data[0].keys())\n",
    "    print(f\"Raw CSV has {len(all_keys)} columns (AEMO includes many fields)\")\n",
    "    print(f\"Sample row keys (first 10): {all_keys[:10]}...\")\n",
    "    print(f\"Sample SETTLEMENTDATE: {test_data[0].get('SETTLEMENTDATE')}\")\n",
    "    \n",
    "    # Verify key fields exist in real data\n",
    "    required_fields = [\"SETTLEMENTDATE\", \"REGIONID\", \"RUNNO\", \"TOTALDEMAND\", \n",
    "                      \"AVAILABLEGENERATION\", \"NETINTERCHANGE\"]\n",
    "    missing = [f for f in required_fields if f not in all_keys]\n",
    "    if missing:\n",
    "        print(f\"\u26a0\ufe0f  WARNING: Missing fields in real data: {missing}\")\n",
    "    else:\n",
    "        print(f\"\u2705 All required fields present in real data\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class NemwebPartition(InputPartition):\n",
    "    \"\"\"\n",
    "    Represents one partition of NEMWEB data.\n",
    "    Each partition handles one region's data.\n",
    "    \"\"\"\n",
    "    def __init__(self, region: str, start_date: str, end_date: str):\n",
    "        self.region = region\n",
    "        self.start_date = start_date\n",
    "        self.end_date = end_date\n",
    "\n",
    "\n",
    "class NemwebReader(DataSourceReader):\n",
    "    \"\"\"\n",
    "    Reader for NEMWEB data source.\n",
    "\n",
    "    The reader has two jobs:\n",
    "    1. partitions() - Plan the work (called on driver)\n",
    "    2. read() - Do the work (called on workers)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, schema: StructType, options: dict):\n",
    "        self.schema = schema\n",
    "        self.options = options\n",
    "        self.regions = options.get(\"regions\", \"NSW1,QLD1,SA1,VIC1,TAS1\").split(\",\")\n",
    "        yesterday = (datetime.now() - timedelta(days=2)).strftime(\"%Y-%m-%d\")\n",
    "        self.start_date = options.get(\"start_date\", yesterday)\n",
    "        self.end_date = options.get(\"end_date\", yesterday)\n",
    "\n",
    "    def partitions(self) -> list[InputPartition]:\n",
    "        \"\"\"\n",
    "        Plan partitions for parallel reading.\n",
    "\n",
    "        SOLUTION 1.2a: Create one partition per region\n",
    "        \"\"\"\n",
    "        partitions = []\n",
    "\n",
    "        for region in self.regions:\n",
    "            partition = NemwebPartition(\n",
    "                region=region.strip(),\n",
    "                start_date=self.start_date,\n",
    "                end_date=self.end_date\n",
    "            )\n",
    "            partitions.append(partition)\n",
    "\n",
    "        return partitions\n",
    "\n",
    "    def read(self, partition: NemwebPartition) -> Iterator[Tuple]:\n",
    "        \"\"\"\n",
    "        Read data for a single partition (runs on workers).\n",
    "\n",
    "        SOLUTION 1.2b: Fetch and parse NEMWEB data\n",
    "        \"\"\"\n",
    "        # Fetch data - fetches REAL current files from AEMO NEMWEB\n",
    "        data = fetch_nemweb_current(\n",
    "            table=\"DISPATCHREGIONSUM\",\n",
    "            region=partition.region,\n",
    "            max_files=6,\n",
    "            use_sample=False,  # Fetch real current files from AEMO\n",
    "            debug=True  # Print debug info\n",
    "        )\n",
    "        \n",
    "        print(f\"[DEBUG] Fetched {len(data)} rows for region {partition.region}\")\n",
    "        if data:\n",
    "            print(f\"[DEBUG] Sample row keys: {list(data[0].keys())}\")\n",
    "            print(f\"[DEBUG] Sample SETTLEMENTDATE value: {data[0].get('SETTLEMENTDATE')}\")\n",
    "\n",
    "        # Convert to tuples matching schema\n",
    "        tuple_count = 0\n",
    "        for row_tuple in parse_nemweb_csv(data, self.schema):\n",
    "            tuple_count += 1\n",
    "            yield row_tuple\n",
    "        \n",
    "        print(f\"[DEBUG] Yielded {tuple_count} tuples for region {partition.region}\")\n",
    "\n",
    "\n",
    "# Test partition planning\n",
    "test_options = {\"regions\": \"NSW1,VIC1,QLD1\"}\n",
    "reader = NemwebReader(schema, test_options)\n",
    "partitions = reader.partitions()\n",
    "\n",
    "print(f\"Created {len(partitions)} partitions (expected: 3)\")\n",
    "for p in partitions:\n",
    "    print(f\"  - Region: {p.region}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Complete the Data Source (5 minutes)\n",
    "\n",
    "Now bring it all together! The DataSource class is the entry point that Spark calls.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def validate_implementation():\n",
    "    \"\"\"Validate the custom data source implementation.\"\"\"\n",
    "    print(\"=\" * 60)\n",
    "    print(\"FINAL VALIDATION\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    checks = {\n",
    "        \"Part 1 - Schema (12 fields)\": False,\n",
    "        \"Part 2 - Partitions\": False,\n",
    "        \"Part 2 - Read\": False,\n",
    "        \"Part 3 - DataSource.name()\": False,\n",
    "        \"Part 3 - DataSource.schema()\": False,\n",
    "        \"Part 3 - DataSource.reader()\": False,\n",
    "    }\n",
    "\n",
    "    # Part 1: Schema\n",
    "    schema = get_dispatchregionsum_schema()\n",
    "    required = [\"TOTALDEMAND\", \"AVAILABLEGENERATION\", \"NETINTERCHANGE\"]\n",
    "    schema_ok = (\n",
    "        len(schema.fields) >= 12 and\n",
    "        all(f in [field.name for field in schema.fields] for f in required)\n",
    "    )\n",
    "    checks[\"Part 1 - Schema (12 fields)\"] = schema_ok\n",
    "\n",
    "    # Part 2: Partitions\n",
    "    reader = NemwebReader(schema, {\"regions\": \"NSW1,VIC1,QLD1\"})\n",
    "    partitions = reader.partitions()\n",
    "    checks[\"Part 2 - Partitions\"] = partitions is not None and len(partitions) == 3\n",
    "\n",
    "    # Part 2: Read\n",
    "    if partitions:\n",
    "        try:\n",
    "            test_partition = NemwebPartition(\"NSW1\", \"2024-01-01\", \"2024-01-01\")\n",
    "            result = list(reader.read(test_partition))\n",
    "            checks[\"Part 2 - Read\"] = len(result) > 0\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    # Part 3: DataSource\n",
    "    try:\n",
    "        checks[\"Part 3 - DataSource.name()\"] = NemwebDataSource.name() == \"nemweb\"\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    try:\n",
    "        ds = NemwebDataSource(options={})\n",
    "        checks[\"Part 3 - DataSource.schema()\"] = len(ds.schema().fields) >= 12\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    try:\n",
    "        ds = NemwebDataSource(options={})\n",
    "        checks[\"Part 3 - DataSource.reader()\"] = ds.reader(schema) is not None\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    # Print results\n",
    "    print()\n",
    "    for check, passed in checks.items():\n",
    "        status = \"\u2705\" if passed else \"\u274c\"\n",
    "        print(f\"  {status} {check}\")\n",
    "\n",
    "    all_passed = all(checks.values())\n",
    "    print()\n",
    "    if all_passed:\n",
    "        print(\"=\" * 60)\n",
    "        print(\"\ud83c\udf89 All checks passed!\")\n",
    "        print(\"=\" * 60)\n",
    "\n",
    "    return all_passed\n",
    "\n",
    "validate_implementation()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Register and Test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "\n",
    "# Note: The tuple-based datasource works for learning, but Spark Connect\n",
    "# (Serverless) has strict datetime serialization requirements. For production,\n",
    "# use the Arrow datasource which uses PyArrow RecordBatches to avoid serialization issues.\n",
    "\n",
    "# Option 1: Test tuple-based datasource (commented out - may have Spark Connect issues)\n",
    "# spark.dataSource.register(NemwebDataSource)\n",
    "# df = (spark.read\n",
    "#       .format(\"nemweb\")\n",
    "#       .option(\"regions\", \"NSW1\")\n",
    "#       .load())\n",
    "\n",
    "# Option 2: Use production Arrow datasource (works perfectly with Spark Connect)\n",
    "from nemweb_datasource_arrow import NemwebArrowDataSource\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "spark.dataSource.register(NemwebArrowDataSource)\n",
    "\n",
    "# Use 2 days ago for CURRENT files (CURRENT has ~7 days of data, but timezone differences mean yesterday may not be available yet)\n",
    "yesterday = (datetime.now() - timedelta(days=2)).strftime(\"%Y-%m-%d\")\n",
    "\n",
    "# Read data using the Arrow datasource - fetches real current files\n",
    "df = (spark.read\n",
    "      .format(\"nemweb_arrow\")\n",
    "      .option(\"table\", \"DISPATCHREGIONSUM\")\n",
    "      .option(\"regions\", \"NSW1\")  # Single region for speed\n",
    "      .option(\"start_date\", yesterday)  # Use recent date for CURRENT files\n",
    "      .option(\"end_date\", yesterday)\n",
    "      .load())\n",
    "\n",
    "# Display results\n",
    "print(f\"Row count: {df.count()}\")\n",
    "display(df.limit(5))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "You built a custom PySpark data source that fetches **live data** from NEMWEB!\n",
    "\n",
    "| Component | Purpose |\n",
    "|-----------|---------|\n",
    "| `DataSource.name()` | Format string for `spark.read.format(...)` |\n",
    "| `DataSource.schema()` | Define output columns and types |\n",
    "| `DataSource.reader()` | Create reader with options |\n",
    "| `DataSourceReader.partitions()` | Plan parallel work units |\n",
    "| `DataSourceReader.read()` | Fetch and yield data (runs on workers) |\n",
    "\n",
    "## Compare to Production\n",
    "\n",
    "Your implementation is a simplified version. The production code in\n",
    "`src/nemweb_datasource_arrow.py` adds:\n",
    "- **PyArrow RecordBatch** for zero-copy transfer (Serverless compatible)\n",
    "- **Volume mode** with parallel downloads to UC Volume\n",
    "- **Multiple tables** (DISPATCHREGIONSUM, DISPATCHPRICE, TRADINGPRICE)\n",
    "- **Retry logic** with exponential backoff\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "Proceed to **Exercise 2** to integrate your data source with Lakeflow Pipelines."
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "dependencies": [
     "/Workspace/Users/david.okeeffe@databricks.com/.bundle/nemweb-lab/dev/files/databricks-nemweb-lab/artifacts/nemweb_datasource-2.10.11-py3-none-any.whl"
    ],
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "01_custom_source_solution",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 4
}